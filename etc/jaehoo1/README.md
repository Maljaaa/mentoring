# ETC

## 블락 논블락/싱크 어싱크

### 블럭 / 논블럭
함수 호출에 대한 이야기. 기술적으로 명확히 구분된다.  
  
**Block**
* 함수 A를 호출했을 때, 함수 A의 수행이 모두 끝날 때 까지 기다린다(Block)
* 함수 A의 수행이 모두 끝나고 리턴되면, 함수 A를 호출한 부분의 다음 부분부터 이어서 실행한다.
* 호출된 함수가 자신이 할 일을 모두 마칠 때까지 제어권을 계속 가지고서 호출한 함수에게 바로 return하지 않으면 Block이다.

**Non-Block**
* 함수 A를 호출했을 때, 함수 A의 실행을 요청하고 바로 리턴된다(Non-Block).
* 호출된 함수가 자신이 할 일을 마치지 않았더라도 바로 제어권을 바로 return하여 호출한 함수가 다른 일을 진행할 수 있도록 하면 Non-Block이다.

### 동기 / 비동기
행위에 대한 이야기. 기술적으로는 구분되지 않지만 추상적으로 구분한다.  
  
작업A와 작업B가 있다고 함  
**Synchronous**
* 작업A가 먼저 모두 처리되고 나서 작업 B가 처리되기 시작하면(하나씩 순차적으로 처리되면) 동기이다.
* 작업A가 작업B를 관찰하는 작업이라면, 작업A와 작업B가 동시에 처리되더라도 동기이다. (여기서 작업A와 작업B를 서로 바꾸어 생각해도 똑같다.)
* 호출된 함수의 수행 결과 및 종료를 호출된 함수 뿐 아니라 호출한 함수도 함께 신경쓰면 동기이다.

**Asynchoronous**
* 작업A와 작업B가 동시에 처리되면 비동기이다.
* 작업A와 작업B는 인과관계가 있어야 한다.
* 작업A와 작업B가 인과관계가 없으면, 동시에 처리되더라도 비동기라고 볼 수 없다.
* 호출된 함수의 수행 결과 및 종료를 호출된 함수 혼자 직접 신경쓰고 처리하면 비동기이다.

### 조합
**블럭 & 동기**  
![image](https://user-images.githubusercontent.com/44667299/163563806-c9df9aed-9019-4364-9aac-addb9ed0c625.png)  
A가 실행되다가 B라는 일을 수행하는 함수를 호출해서 B를 시작한다. B라는 일이 끝나면 함수를 리턴한다. A와 B는 순차적으로 진행되기 때문에 동기이며,  B라는 일을 하는 함수를 호출하고 그 일이 끝나고 나서야 리턴되므로 블럭된 것이다. 따라서 블럭/동기
>손님 : 아메리카노 주문  
>직원 : 아메리카노 만드는 중  
>손님 : (그 자리에 서서 기다리는 중. 결과가 궁금함. 테이블 못가고 서있음.)

<br/>

**블럭 & 비동기**  
![image](https://user-images.githubusercontent.com/44667299/163564201-001675ce-4280-4e0e-8d1d-f7069f3c77af.png)  
A는 B라는 일을 시킨다. 그리고 바로 리턴하고 (여기서는 논블럭)  B는 일을 시작하고, A도 자신의 일을 한다. A는 중간에 B라는 일이 하는 중간 결과를 보고 받아서 처리해야한다. A는 B에게 요청을 해서 중간결과를 기다린다(블록), 요청의 결과를 받고 나서 그 결과를 이용해서 A는 자신의 일을 처리한다. 동시에 B 는 또 자신의 일을 동시에 한다. (비동기) A는 다시 B에게 중간결과를 요청해서 기다린다 (블록) , 요청의 결과를 받고 A는 자신의 일을, B는 자신의 일을 한다. 반복된다.  
  
이 글을 읽고, 사실 갸우뚱 해야한다. 중간에 블록되는 동안에는 "동기" 라고 말할 수 있기 때문이다. 즉 어느 한 순간에 대해 해석하자면 틀릴 수도 있는 것이다. "정답"이 존재하지 않는다. 다만 이런 패턴들이 분명히 사용되고 있구나라고 감을 잡는게 목적이다.  
  
블럭 & 비동기는 결국 다른 작업이 끝날 때를 기다려야 하기 때문에 동기 & 비동기와 비슷한 효율이 나온다.
>손님 : 아메리카노 주문  
>직원 : 아메리카노 만드는중  
>손님 : (안궁금함. 테이블에 가고싶지만 못가고 서있음.)

<br/>

**논블럭 & 동기**  
![image](https://user-images.githubusercontent.com/44667299/163564556-a463f3ac-de02-431a-95aa-88ccea5ad7f3.png)  
A는 B라는 일을 시킨다. 바로 리턴한다. (논블럭) B는 일을 시작하는데, A는 자신의 일을 하지 않는다. A의 하는 일이란 그저 B가 하는 일을 확인하는 것이다. B가 결과 보고(중간 보고가 아니다) 를 했는지를 확인하는 함수를 호출하고, 바로 리턴한다 (논블럭) 즉 결과 보고를 받을 때 까지 기다리는게 아니라, 결과 보고가 나왔는지 확인하고 바로 리턴하는 것이다. 이 짓을 계속한다. 즉 함수를 계속 논블럭으로 호출되긴 하나, A는 그저 B를 염탐할 뿐이다. 이 상태를 말한다. 그냥 염탐하지 말고 B가 일을 모두 끝마치고 리턴되길 기다리지(그냥 블럭/동기로 하는게 나은 상황이 연출된다)  
  
이후에 B가 결과보고를 하면, B는 자신의 일이 끝난 것이고 A는 이제서야 자신의 일을 처리하게 된다. 즉 순차적이라는 말이다. 따라서 동기  
  
논블로킹으로 자신의 작업을 계속하고 있지만 다른 작업과의 동기를 위해 계속해서 다른 작업이 끝났는지 조회한다. 즉, 호출된 함수가 호출한 함수에게 제어권을 바로 return하여 호출한 함수가 다른 업무를 볼 수 있었음(Non-Blocked)에도 불구하고, 여전히 호출된 함수의 업무 결과에만 계속 함께 신경쓰느라(Synchronous) 제 할일을 못하게 되는 현상이 일어난다. 논블럭 & 동기도 효율이 좋지 않다.
>손님 : 아메리카노 주문  
>직원 : 아메리카노 만드는중  
>손님 : 재촉

<br/>

**논블럭 & 비동기**  
![image](https://user-images.githubusercontent.com/44667299/163565103-59e46cb6-9f78-4626-95eb-41cc4cad0778.png)  
A는 B의 일을 시작시키고 바로 리턴한다. (논블럭) 그리고 A와B는 각자 자신의 일을 한다. (비동기)  
  
자원이 충분하다면 효율이 좋다. 자신의 작업이 멈추지도 않고, 다른 주체가 하는 작업의 결과가 나왔을 때 콜백을 설정하기도 한다. 다른 주체에게 작업을 맡겨놓고 자신이 하던 일을 계속할 수 있기 때문에 해야 할 작업이 대규모이고, 동기가 필요하지 않을 때 효과적이다.
>손님 : 아메리카노 주문  
>직원 : 아메리카노 만드는 중  
>손님 : 자리가서 SNS, 유튜브 시청  
>직원 : 아메리카노 제작 완료

<br/>

출처 : https://hamait.tistory.com/930  
사진 출처 : https://velog.io/@leehyunho2001/%EB%8F%99%EA%B8%B0-%EB%B9%84%EB%8F%99%EA%B8%B0-%EB%B8%94%EB%9F%AD-%EB%84%8C%EB%B8%94%EB%9F%AD

### 결론
정답은 없다.  
  
1 구분하지 않는다.  
  
블럭/동기는 같이 말하며, 논블럭/비동기도 함께 묶어 말한다.  
애초에 구분할 필요가 없다.  

<br/>

2 굳이 구분할 경우  
  
블럭/논블럭은 동시성과는 무관한 이야기이다. 단지 메서드 호출한 후로 시간이 오래 걸리면 블로킹이다. 즉 메서드가 얼마나 오래 걸리냐의 문제로 블록과 논블록은 귀결된다.

## WebServer VS WebApplicationServer

### WebServer
웹 브라우저의 요청을 받아 HTTP를 통해 웹 브라우저에서 요청하는 HTML 문서나 오브젝트(이미지 파일 등)를 전송해주는 서버  
>예 : Apache HTTP Server, Microsoft Internet Information Service, Google Web Server 등

### WebApplicationServer
사용자에게 동적 서비스를 제공하기 위해 웹 서버로부터 요청을 받아 데이터 처리를 수행하거나, 웹 서버와 데이터베이스 서버 또는 웹 서버와 파일 서버 사이에서 인터페이스 역할을 수행하는 서버
>예 : Oracle WebLogic, Apache Tomcat, IBM WebSphere, JEUS 등

### WebServer가 필요한 이유?
* 클라이언트(웹 브라우저)에 이미지 파일(정적 컨텐츠)을 보내는 과정
  * 이미지 파일과 같은 정적인 파일들은 웹 문서(HTML 문서)가 클라이언트로 보내질 때 함께 가는 것이 아니다.
  * 클라이언트는 HTML 문서를 먼저 받고 그에 맞게 필요한 이미지 파일들을 다시 서버로 요청하면 그때서야 이미지 파일을 받아온다.
  * Web Server를 통해 정적인 파일들을 Application Server까지 가지 않고 앞단에서 빠르게 보내줄 수 있다.
* 따라서 Web Server에서는 정적 컨텐츠만 처리하도록 기능을 분배하여 서버의 부담을 줄일 수 있다.

### WAS가 필요한 이유?
* 웹 페이지는 정적 컨텐츠와 동적 컨텐츠가 모두 존재
  * 사용자의 요청에 맞게 적절한 동적 컨텐츠를 만들어서 제공해야 한다.
  * 이때, WebServer만을 이용한다면 사용자가 원하는 요청에 대한 결과값을 모두 미리 만들어 놓고(정적 컨텐츠) 서비스를 해야 한다.
  * 하지만 이렇게 수행하기에는 자원이 절대적으로 부족하다.
* 따라서 WAS를 통해 요청에 맞는 데이터를 DB에서 가져와서 비즈니스 로직에 맞게 그때 그때 결과를 만들어서 제공함으로써 자원을 효율적으로 사용할 수 있다.

### 그렇다면 WAS가 Web Server의 기능도 모두 수행하면 되지 않을까? X
* 기능을 분리하여 서버 부하 방지
  * WAS는 DB 조회나 다양한 로직을 처리하느라 바쁘기 때문에 단순한 정적 컨텐츠는 WebServer에서 빠르게 클라이언트에 제공하는 것이 좋다.
  * WAS는 기본적으로 동적 컨텐츠를 제공하기 위해 존재하는 서버이다.
  * 만약 정적 컨텐츠 요청까지 WAS가 처리한다면 정적 데이터 처리로 인해 부하가 커지게 되고, 동적 컨텐츠의 처리가 지연됨에 따라 수행 속도가 느려진다.
  * 즉, 이로 인해 페이지 노출 시간이 늘어나게 될 것이다.
* 물리적으로 분리하여 보안 강화
  * SSL에 대한 암복호화 처리에 WebServer를 사용
* 여러 대의 WAS를 연결 가능
  * Load Balancing을 위해서 WebServer를 사용
  * fail over(장애 극복), fail back 처리에 유리
  * 특히 대용량 웹 어플리케이션의 경우(여러 개의 서버 사용) WebServer와 WAS를 분리하여 무중단 운영을 위한 장애 극복에 쉽게 대응할 수 있다.
  * 예를 들어, 앞 단의 WebServer에서 오류가 발생한 WAS를 이용하지 못하도록 한 후 WAS를 재시작함으로써 사용자는 오류를 느끼지 못하고 이용할 수 있다.
* 여러 웹 어플리케이션 서비스 가능
  * 예를 들어, 하나의 서버에서 PHP Application과 Java Application을 함께 사용하는 경우
* 기타
  * 접근 허용 IP 관리, 2대 이상의 서버에서의 세션 관리 등도 WebServer에서 처리하면 효율적이다.

즉, 자원 이용의 효율성 및 장애 극복, 배포 및 유지보수의 편의성을 위해 Web Server와 WAS를 분리한다.  
WebServer를 WAS 앞에 두고 필요한 WAS들을 WebServer에 플러그인 형태로 설정하면 더욱 효율적인 분산 처리가 가능하다.

>failover : 시스템, 서버, 네트워크가 이상이 생겼을 경우 예비시스템으로 전환되는 기능  
>failback : failover에 따라 전환된 서버/시스템/네트워크를 장애 발생전으로 되돌리는 처리

## Monolithic service Application VS Micro Service Application
MSA는 Micro Service Architecture의 줄임말로서 하나의 큰 어플리케이션을 여러 개의 작은 어플리케이션으로 나눠 만드는 아키텍쳐다. MSA가 등장하기 이전에는 하나의 서비스는 하나의 어플리케이션으로 만드는 것이 일반적이었고, 이를 Monolithic architecture, 모놀리식 아키텍쳐 라고 한다.

### Monolithic Architecture
하나의 프로젝트에 대해서 하나의 어플리케이션이 대응. 때문에 이 어플리케이션은 큰 규모를 이루게 된다.  
  
장점
* 로컬 환경에서 개발이 편리
* 통합 시나리오 테스트 진행이 수월
* 배포가 간편

단점
* 코드의 수정 및 추가가 힘듦
* 효율적인 자원 관리가 힘듦
* 자주 업데이트 불가능
* 새로운 기술 적용이 힘듦
* 부분의 장애가 서비스 전체적인 장애
* scale out이 불가능
>scale out  
>서버를 운영하다보면 갑작스런 이용자의 증가, 사업 확장 등의 이유로 더 많은 서버 용량과 성능이 필요하게 된다. 이럴 경우 서버를 확장시키는 여러 방법이 있는데 scale out은 서버를 여러 대 추가하여 확장하는 방법이다.  
>만약 기능 별로 분리가 되어 있다면 수강신청의 경우, 로그인하는 서버와 수강신청을 하는 서버만 확장을 시킴으로서 많은 트래픽을 견딜 수 있지만 기능 별로 분리가 안 되어 있다면 모든 서버를 통째로 추가해야 할 것  
>ex : 대학교의 수강신청 등

장점과 단점 모두 Monolithic Architecture의 공통적인 특징, 단순함 때문에 야기된다.  
프로젝트가 하나의 어플리케이션으로 이루어져 있으니 개발하기도 편하고, 전체적인 테스트가 수월하다. 당연히 배포도 하나의 어플리케이션만 하면 된다.  
하지만 프로젝트 규모가 점점 커질수록 단순함은 독이 된다.  
너무 많은 기능들이 하나의 어플리케이션에 묶여있다보니 서로 의존성이 높아지고 이는 개발자들에게 있어서는 이해하기 어려운 코드가 될 것  
  
이러한 장단점이 극명하기 때문에 Monolithic Architecture가 좋지않다고만 볼 수는 없다.  
실제로 아직 많은 소프트웨어가 Monolithic 방식으로 개발되고 유지되고 있으며 특히 소규모 프로젝트의 경우에는 특별히 나눌 필요없이 Monolithic Architecture가 더 합리적일 수 있다.  
하지만 프로젝트의 규모가 커지고 한 프로젝트에 속한 개발자가 많아질수록 부적합해진다.  
최근에 등장하는 프로젝트들은 모두 규모가 크고 많은 기능들을 필요로 하고, 그렇기 때문에 MSA가 등장하였고 더 각광받은 것

### Micro Service Architecture
장점
* 빌드 및 테스트 시간을 단축  
물론 처음부터 끝까지 서비스를 빌드하고 테스트하는 시간은 같다.  
하지만 개발을 하다보면 부분적으로 테스트를 해야하는 순간도 필요할텐데 MSA는 이를 가능하게 하여 시간을 절약할 수 있음
* 유연하게 기술을 적용  
서버가 분리되다보니 각 서버의 언어와 프레임워크도 유연하게 가져갈 수 있다.  
auth 서버는 django, 채팅서버는 node로 구현하는 방식으로 말이다.
* scale out 가능
* 서비스간의 연관성 낮음  
한 서버에서의 문제가 다른 서버에 영향을 미치지 않게 되므로 긍정적인 부분
  
단점
* 성능 이슈  
Monolithic의 경우 다른 기능을 호출할 경우 같은 프로젝트 내에서 method 호출로 끝남  
이는 단순히 메모리 안에서 일어나 매우 빠르고 문제의 소지가 적음. 다른 서비스 간에 Network 통신(주로 http)을 주고 받게 됨.  
method를 호출하는 것과는 많은 차이가 있다.  
Micro Service Architecture의 경우에는 이렇게 단순하게 처리할 수 없다.
* 트랜잭션이 불편  
서버가 나뉘고 다른 서버 간의 트랜잭션 처리를 해야할 경우, 불편함이 따른다.  
트랜잭션을 위한 로직이 또 필요해짐
* 개발 시간 증가  
MSA는 서버가 분리 됨에 따라 이에 따라 관리가 필요.  
DB가 서버에 맞춰 증가할 수도 있고, 로깅, 모니터링, 배포, 테스트 등 여러 관리에 신경을 더 써야함

### 결론
각 프로젝트의 상황에 맞게 적절한 아키텍쳐를 골라야함  
  
출처 : https://ssungkang.tistory.com/entry/MSA-Monolithic-Architecture-VS-Micro-Service-Architecture

## Multi Module VS MSA

### Multi Module
서로 독립적인 프로젝트(인증, 어플리케이션)를 하나의 프로젝트로 묶어 모듈로서 사용되는 구조. 멀티 모듈을 사용하면 공통적인 기능을 모아 하나의 모듈로 만드는 것이 가능하다. 즉, 인증과 어플리케이션에서 공통으로 사용하는 util, domain, Repository등을 모듈로 분리해 사용할 수 있는 것  
* MSA와 멀티 모듈은 다름  
멀티 모듈로 이루어진 마이크로 서비스가 있고, 이 서비스들 간에 구성되어 있는 형태가 MSA
* 모놀릭스는 멀티모듈을 못쓰나? → no  
쓸 수 있음
* 멀티 모듈은 모놀릭스 <-> MSA 간 서로 전환하는 작업을 수월하게 할 수 있는 구조가 되도록 도와줌

장점
* 재사용, 공유 할 수 있음  
멀티 모듈의 각 모듈들을 독립적이고 필요한 최소 의존성을 가지고 있기 때문에 다른 영향을 고려하지 않고 재사용이 가능
* 빌드를 쉽게 할 수 있음  
멀티 프로젝트의 경우에는 각 프로젝트마다 빌드를 해줘야 한다. 하지만 멀티 모듈은 최상위 모듈에서 전체 프로젝트를 빌드 할 수 있다는 점이 큰 장점. 모듈 별도 빌드와 테스트도 아주 큰 장점이다. 싱글 모듈인 경우에는 한 가지 기능만 수정했다해도 전체를 빌드해야한다. 전체 빌드와 한 모듈 빌드 차이
* 변경으로 인한 영향 최소화  
버그를 발견하면 전체 시스템이 아닌 버그가 포함된 모듈만 업데이트하면 된다. 그로 인해 수정으로 인한 영향이 최소화 됨. 한 곳을 수정했는데 예상치 못한 곳에서 다른 에러가 터지는 불상사들이 다 얽히고 설킨 의존성 때문. 전체를 빌드, 재배포할 필요없이 편하게 버그 모듈만 빌드 재배포할 수 있는 장점도 있다.
* 하나의 모듈을 업데이트할 때 관련 프로젝트 전체를 이해할 필요가 없다.  
각 모듈이 갖는 책임과 역할이 명확해 리팩토링, 기능 변경 영향 파악하기가 쉬워졌기 때문
* 의존성을 최소화  
의존성을 최소화하면 변경으로 인한 영향을 최소화할 수 있다. 즉 결합도가 낮아짐. 또한 각 모듈을 가볍게 유지해서 빌드 시간을 줄이고 생산성을 향상시킬 수 있음
  
단점
* 멀티 모듈 학습에 대한 비용
* 여러 모듈을 유지 관리하기가 더 어려울 수 있음
* 기능이 추가될수록 처음 설계와 다르게 의존성이 꼬일 가능성

### 결론
Monolithic → MSA로 한 번에 가긴 힘들다.  
프로젝트가 커지면 커질수록 멀티 모듈은 거의 필수가 된다고 한다. 하지만 크기가 작은 프로젝트의 경우에는 멀티 모듈의 장점을 살릴 수 없는 의미없는 멀티 모듈이 될 확률이 높다.

## NginX VS Apache
예전에는 웹 서버로 Apache를 많이 사용하였지만 최근에는 Nginx를 사용하는 추세  
어떤 이유 때문에 Nginx를 점점 더 사용하는 것일까?  
Nginx가 트래픽이 많은 웹사이트에 적합하기 때문  
  
Nginx는 **대용량 트래픽을 처리하기 위해 가벼움과 높은 성능을 목표로 하는 경량 웹 서버**이다.  
  
초기에는 정적 파일을 제공하는 웹 서버로 Apache를 보조하는 역할을 수행하였지만 오늘날에는 리버스 프록시, 로드 밸런서, 메일 프록시 및 HTTP 캐싱 등 전체 범위의 서버 작업을 처리하는 웹 서버로 발전하였다.  
  
즉, Nginx 웹 서버는 Apache 웹 서버의 성능 제한을 해결하기 위해 탄생한 웹 서버이다.

### 설계 아키텍처의 차이
**Nginx**
* 이벤트 중심 접근 방식으로 하나의 스레드 내에서 여러 요청을 처리하는 구조
* 비동기 Event-Driven 구조 : Event Handler에서 비동기 방식으로 먼저 처리되는 요청을 진행 
* 코어 모듈이 Apache보다 적은 리소스로도 많은 트래픽을 효율적으로 처리 가능
![image](https://user-images.githubusercontent.com/44667299/163663954-fe35e8ca-6a51-47e3-bc2e-01a4a3718bfa.png)  
  
Nginx는 Event-Driven 구조로 동작하기 때문에 한 개 또는 고정된 프로세스만 생성하여 사용하고, 비동기 방식으로 요청들을 Concurrency 하게 처리할 수 있다. Nginx는 새로운 요청이 들어오더라도 새로운 프로세스와 쓰레드를 생성하지 않기 때문에 프로세스와 쓰레드 생성 비용이 존재하지 않고, 적은 자원으로도 효율적인 운용이 가능하다. 이러한 Nginx의 장점 덕분에 단일 서버에서도 동시에 많은 연결을 처리할 수 있다.  
  
**Apache**
* 프로세스 기반 접근 방식으로 하나의 스레드가 하나의 요청을 처리하는 구조
* 매 요청마다 스레드를 생성 및 할당해야 하기 때문에 리소스를 많이 잡아먹음
![image](https://user-images.githubusercontent.com/44667299/163664074-5c943864-33b8-4522-b50d-15f456577579.png)  
  
Apache는 클라이언트로부터 받은 요청을 처리할 때 새로운 프로세스 또는 쓰레드를 생성하여 처리한다. 요청마다 쓰레드가 생성되므로 접속하는 사용자가 많으면 그만큼 쓰레드가 생성되어 CPU와 메모리 자원의 소모가 커진다.

### 성능 차이
Nginx와 Apache 두 웹 서버 모두 정적 및 동적 컨텐츠를 제공하는 방식이 다르다.  
  
**정적 컨텐츠**
* 서버 PC의 디스크에 저장하는 파일 기반 방법으로 정적 컨텐츠 제공
* 설계 아키텍처 구조상 Nginx가 적은 비용으로 효율적으로 제공

**동적 컨텐츠**
* 두 웹 서버 모두 서버 자체에서 동적 컨텐츠 처리 가능
* Nginx는 SCGI 핸들러와 FastCGI 모듈을 사용해서 동적 컨텐츠 제공할 수 있음
* 동적 컨텐츠는 두 웹 서버 성능이 비슷함

### OS 지원 여부
**Nginx**
* 거의 모든 Unix 계열 OS 지원
* Windows는 부분적으로 지원

**Apache**
* Linux 및 BSD를 포함한 모든 Unix 계열 OS 지원
* Windows 모두 지원

### 분산/중앙 집중식 구성
**Nginx**
* 추가 구성을 허용하지 않음
* 권한이 없는 사용자가 웹 사이트의 특정 측면을 제어할 수 없지만 추가 구성을 제공하지 않음으로써 성능 향상
* 디렉토리 구성을 허용하지 않음으로 .htaccess 파일을 검색하고 사용자가 만든 요구 사항을 해석할 필요 없기 때문에 Apache보다 빠르게 요청을 처리할 수 있음

**Apache**
* .htaccess 파일을 통해 디렉토리 별로 추가 구성을 허용
* 이로 인해 권한이 없는 사용자가 웹 사이트의 특정 측면을 제어할 수 있음

### 요청을 처리 및 해석하는 방법의 차이
**Nginx**
* 요청을 해석하기 위해 URI를 전달
* URI로 전달함으로써 웹 서버뿐만 아니라 프록시 서버, 로드 밸런서 및 HTTP 캐시로 쉽게 동작 가능
* 서버에서 클라이언트로 데이터가 전송되는 속도가 Apache보다 더 빠름

**Apache**
* 요청을 해석하기 위해 파일 시스템 위치 전달
* URI 위치를 사용하지만 일반적으로 더 추상적인 디렉토리 구조를 사용

### 기능 모듈의 차이
**Nginx**
* 타사 플러그인 과정으로 선택되고 컴파일되기 때문에 동적으로 모듈을 로드할 수 없음
* 따라서 사용하려는 기능만 선택해서 서버를 실행 = 가벼움

**Apache**
* 동적으로 로드 가능한 다양한 60개의 공식 모듈을 제공
* 모든 모듈을 가지고 서버가 실행되지만 실제 사용되는 모듈을 소수임 = 무거움

### 유연성
**Nginx**
* 아직까지는 동적 모듈과 로딩을 지원하지 않음

**Apache**
* 동적 모듈, 로딩 지원

### 보안
두 웹 서버 모두 C언어 기반으로 확장된 보안을 제공. 하지만 Nginx가 코드가 더 작기 때문에 미래 지향적인 보안 관점에서 장점을 가진다.  
  
즉, 비슷하지만 Nginx가 조금 더 안전한 것으로 간주

### 결론
Apache는 .htacess 파일을 제공하기 때문에 이를 활용하거나 Nginx에게 없는 핵심 모듈을 사용할 경우 Apache를 사용하며, 빠른 정적 컨텐츠를 처리하고 싶고 대용량 트래픽을 처리하는 웹 사이트인 경우는 Nginx를 사용하면 된다.  
또한, 두 서버를 함께 사용해도 된다. Apache 앞단에 Nginx를 프록시 서버로 활용할 수 있다.  
  
Apache는 Nginx 에 비해 모듈이 다양하다.  
Apache는 안정성, 확장성, 호환성을 장점으로 들자면, Nginx는 성능이 우세하다는 장점이 있다.  
  
Apache와 NginX는 모두 강력하고 유연한 웹 서버이다. 어떤 웹 서버가 더 좋다 라는 결론은 없다.  
상황과 비용에 따라, 혹은 안정성이나 효율성에 따라 적합한 웹 서버를 사용

|Apache|NginX|
|---|---|
|요청 당 스레드 또는 프로세스가 처리하는 구조|비동기 이벤트 기반으로 요청|
|CPU/메모리 자원 낭비 심함|CPU/메모리 자원 사용률 낮음|
|NginX보다 모듈이 다양|Apache에 비해 다양한 모듈이 없음|
|PHP 모듈 등 직접 적재 가능|많은 접속자들 대응 가능|
|안정성, 확장성, 호환성 우세|성능 우세|
|동적 컨텐츠 단독 처리 가능|동적 컨텐츠 단독 처리 불가능|
  
참고 : https://sorjfkrh5078.tistory.com/289

## Servlet VS Netty
### Servlet
Dynamic Web Page를 만들 때 사용되는 자바 기반의 웹 애플리케이션 프로그래밍 기술. 클라이언트의 요청을 처리하고, 그 결과를 반환하는 Servlet 클래스의 구현 규칙을 지킨 자바 웹 프로그래밍 기술. 웹 요청과 응답의 흐름을 간단한 메서드 호출만으로 체계적으로 다룰 수 있게 해주는 기술  
  
자바 클래스로 웹 애플리케이션을 작성한 뒤 이후 웹 서버 안에 있는 웹 컨테이너에서 이것을 실행하고, 웹 컨테이너에서는 서블릿 인스턴스를 생성 후 서버에서 실행되다가 웹 브라우저에서 서버에 요청(Request)을 하면 요청에 맞는 동작을 수행하고 웹 브라우저에 HTTP형식으로 응답(Response)한다.  
  
**특징**
* 클라이언트의 Request에 대해 동적으로 작동하는 웹 애플리케이션 컴포넌트
* HTML을 사용하여 Response 한다.
* JAVA의 스레드를 이용하여 동작
* MVC 패턴에서의 컨트롤러로 이용됨
* HTTP 프로토콜 서비스를 지원하는 javax.servlet.http.HttpServlet 클래스를 상속받음
* UDP보다 속도가 느리다.
* HTML 변경 시 Servlet을 재 컴파일해야 하는 단점

**서블릿 컨테이너**  
서블릿을 담고 관리해주는 컨테이너.  구현되어 있는 servlet 클래스의 규칙에 맞게 서블릿은 관리해주며 클라이언트에서 요청을 하면 컨테이너는 HttpServletRequest, HttpServletResponse 두 객체를 생성하며 post, get여부에 따라 동적인 페이지를 생성하여 응답을 보낸다.  
  
**서블릿 컨테이너 주요 기능**
* 생명주기 관리
* 통신 지원
* 멀티스레딩 관리
* 선언적인 보안관리

**Servlet을 쓰는 이유**  
서블릿을 사용하지 않고 직접 HTTP 통신으로 오고가는 문자열을 파싱하여 서블릿과 같은 기능을 구현해도 무방하지만, 이미 편리하게 사용할 수 있는 서블릿을 놔두고 직접 문자열 파싱을 구현하는 것은 개발자가 온전히 비즈니스 로직에 집중하지 못하게 만들 수 있다.  
서블릿을 통해 문자열 파싱 등에 열올리지 않고 비즈니스 로직에 더욱 집중할 수 있다.

### Netty
TCP, UDP 소켓 서버 개발과 같은 네트워크 프로그래밍을 매우 간단하고 능률적으로 만들어주는 비동기 이벤트 드리븐 자바 네트워크 프레임워크  
  
단순히 네트워크 통신과 관련된 기능을 제공할 뿐만 아니라 일반적으로 네트워크 애플리케이션에서 사용하는 다양한 기술들을 포함하고 있다. 덕분에 자바 프로그래머들은 네트워크 프로그래밍이나 멀티스레드와 관련된 처리보다는 자신들의 비즈니스 로직에 좀 더 집중할 수 있게 됨. 네티를 이용한 애플리케이션은 최소 10만개 이상의 클라이언트 커넥션을 처리할 수 있을 정도로 안정되어 있다.  
  
Netty는 자바 개발자가 네트워크 애플리케이션을 빠르고 쉽게 개발할 수 있도록 다양하고 강력한 기능들을 제공한다.  
  
**특징**
* Asynchronous IO (비동기 입출력)
  * Netty 프레임워크는 비동기처리를 지향
* Event Model
  * 입출력을 위해 잘 정의된(Well-defined) 이벤트 모델을 가지고 있음
  * 사용자가 코어 로직을 손대지 않고도 직접 이벤트 타입을 구현할 수 있도록 지원한다. 각 이벤트 타입은 엄격하게 계층화되어 있어 서로 잘 구분된다. 다른 NIO 프레임워크들은 이벤트 타입을 추가할 경우 기존 코드에 영향을 주거나 아예 커스텀 이벤트 추가를 막는 경우도 많다.
* Universal Asynchoronous I/O API
  * 자바의 전통적인 IO API는 TCP, UDP에 따라 다른 형태의 API를 제공한다. 이 때문에 TCP 애플리케이션을 UDP로 포팅하는게 마냥 쉽지만은 않다.
  * Netty는 Channel이라는 Async I/O 인터페이스를 가지고 있다. 이 채널은 Point-to-Point 통신을 추상화한 개념이다. 일단 Netty앱을 개발하면 추상화를 통해서 실제 통신 부분과 상관없이 로직을 개발할 수 있다. Netty는 여러 개의 필수적인 transports를 하나의 API를 통해서 제공한다.
    * NIO 기반의 TCP/IP
    * Old IO 기반의 TCP/IP
    * Old IO 기반의 UDP/IP
    * Local transport
  * 이 transport 사이를 전환하는 것은 팩토리 클래스인 ChannelFactory를 고르는 몇 줄만 고치면 된다. 심지어는 아직 구현되지 않은 transport를 사용하는 클라이언트 코드를 미리 구현할 수도 있다. transport 역시 직접 구현해서 사용할 수 있다.
* ChannelBuffer
  * Netty는 독자적인 버퍼를 구현했다. 독자적인 버퍼를 구현해서 제로카피(Zero Copy)도 지원하고, ByteBuffer보다 빠른 성능을 구현했다. 다이나믹 버퍼 타입도 지원한다.
* 기타
  * 그 밖에 빠른 개발을 위한 수 많은 컴포넌트들을 가지고 있음
  * 단순하든 복잡하든 바이너리든 아니든 상관없이 프로토콜 코덱을 작성할 때 마주하게 될 대부분의 이슈를 해결할 수 있도록 기본적인 코덱과 고급 코델을 제공
  * NIO 이면서도 SSL을 지원
  * Google Protobuf 와 통합이 가능

>제로카피(Zero Copy) : 컴퓨터 시스템에서 CPU의 개입을 받지 않고 한 메모리의 영역에서 다른 메모리의 영역으로 데이터를 카피하는 작업을 말한다. 이는 CPU 사이클을 절약하여서 네트워크나 SSD와 같이 빠른 데이터가 필요한 시스템에서 유용하게 이용

**Netty를 쓰는 이유**  
* 성능이 좋음
  * Non-blocking Asynchronous가 기본
  * 적은 thread로 많은 요청 처리
  * GC부하를 최소화하는 Zero-copy ByteBuf 지원
* 유연하고 쓰기 쉬움
  * 각종 network protocol 기본 지원
  * 필요한 부분을 쉽게 조립해 쓸 수 있는 구조
  * multithread 처리 없이도 사용 가능

## 로드밸런싱
* 서버에 가해지는 부하를 분산해주는 장치 또는 기술
* 클라이언트와 서버풀 사이에 위치해 한 대의 서보로 부하가 집중되지 않도록 트래픽을 관리
* 각각의 서버가 최적의 퍼포먼스를 보일 수 있도록 관리
* 스케일 아웃으로 시스템 확장을 했다면, 반드시 동반되어야하는 기술

### 로드 밸런싱 알고리즘
**라운드 로빈 방식(RR, Round Robin)**
* 서버에 들어온 요청을 순차적으로 배정
* 각 서버가 동일한 스펙을 가지고 있고, 세션이 오래 지속되지 않는 경우에 적합

**가중 라운드로빈 방식(Weighted Round Robin)**
* 각각의 서버마다 가중치를 매기고 가중치가 높은 서버에 우선적으로 배정
* 각각의 서버의 트래픽 처리 속도가 상이한 경우에 적합

**IP 해시 방식(IP Hash)**
* 클라이언트의 IP 주소를 해시 함수를 통해 특정 서버에 매핑
* 사용자가 항상 동일한 서버로 연결되는 것을 보장

**최초 연결 방식(Least Connection)**
* 요청이 들어온 시점에 가장 적은 연결 상태를 보이는 서버에 우선적으로 배정
* 서버에 분배된 트래픽이 일정하지 않은 경우 적합

**최소 응답시간 방식(Least Response Time)**
* 서버의 현재 연결 상태와 응답시간을 모두 고려해 트래픽 배분

### 로드 밸런서 종류
* OSI 7 계층에 따라 L1 로드밸런서부터 L7로드 밸런서까지 존재
* L4 로드밸런서부터 포트 정보를 바탕으로 로드를 분산하는 것이 가능하므로 L4와 L7이 가장 많이 활용
* 한 대의 서버에 다수의 서버 프로그램을 운영하는 경우 최소 L4 이상의 로드밸런서를 사용해야함

**L4 로드밸런서**
* 네트워크 계층(IP)이나 전송 계층(TCP, UDP)의 정보를 바탕으로 로드 분산
* IP 주소, 포트번호, Mac 주소, 전송 프로토콜에 따라 로드 분산
* (+) 데이터를 확인하지 않고 패킷레벨에서만 로드를 분산하기 때문에 빠르고 효율적
* (+) 데이터의 내용을 복호화할 필요가 없으므로 안전
* (+) 저렴
* (-) 섬세한 라우팅 불가
* (-) 사용자의 IP가 수시로 바뀌는 경우 연속적인 서비스 제공 불가

**L7 로드 밸런서**
* 애플리케이션 계층(HTTP, FTP, SMTP)에서 로드를 분산
* HTTP 헤더, 쿠키 등과 같은 사용자 요청을 기준으로 특정 서버에 트래픽 분산 가능
* 즉, 패킷의 내용을 확인하고, 그 내용에 따라 로드를 분산 가능
* (+) 클라이언트의 요청을 보다 세분화해 서버에 전달
* (+) 캐싱 기능 제공
* (+) DOS같은 비정상적인 트래픽을 필터링할 수 있어 안전성이 높음
* (-) 패킷 내용을 복호화 해야하므로 높은 비용
* (-) 클라이언트와 로드밸런서가 인증서를 공유해야하기 때문에 보안상의 위험이 존재

참고 : https://dheldh77.tistory.com/entry/%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC-%EB%A1%9C%EB%93%9C-%EB%B0%B8%EB%9F%B0%EC%8B%B1Load-Balancing?category=823110

## Reverse Proxy
애플리케이션 서버의 앞에 위치하여 클라이언트가 서버를 요청할 때 리버스 프록시를 호출하고, 리버스 프록시가 서버로부터 응답을 전달받아 다시 클라이언트에게 전송하는 역할  
  
이 경우, 클라이언트는 애플리케이션 서버를 직접 호출하는 것이 아니라 프록시 서버를 통해 호출하기 때문에 리버스 프록시는 애플리케이션 서버를 감추는 역할  
![image](https://user-images.githubusercontent.com/44667299/163663749-2f23cdbe-7526-4f14-83a0-886df1effeb7.png)  
사진 출처 : https://sujinhope.github.io/2021/06/13/Network-%ED%94%84%EB%A1%9D%EC%8B%9C(Proxy)%EB%9E%80,-Forward-Proxy%EC%99%80-Reverse-Proxy.html  
  
### 리버스 프록시의 특징/역할
* 로드밸런싱 : 리버스 프록시 뒤에 여러 개의 WAS를 둠으로써, 사용자 요청을 분산할 수 있다. End-point 마다 호출 서버를 설정할 수 있어 역할에 따라 서버의 트래픽을 분산할 수도 있다.
* 보안 : 보안 상의 이유로 서버에 직접 접근하는 것을 막기 위해 DMZ같은 네트워크에 리버스 프록시를 구성하여 접근하도록 한다.
* 속도와 안전성
* 신뢰성 증대

## CDN
* 느린 응답 속도/다운로딩 시간을 극복하기 위한 기술
* 지리적, 물리적으로 떨어져 있는 사용자에게 컨텐츠를 더 빠르게 제공하기 위한 기술
* 사용자와 가까운 곳에 위치한 Cache 서버에 해당 컨텐츠를 저장하고 컨텐츠 요청 시 Cache 서버가 응답

각 지역에 캐시 서버(PoP, Points of presence)를 분산 배치해, 근접한 사용자의 요청에 원본 서버가 아닌 캐시 서버가 콘텐츠를 전달함

## 캐시
데이터나 값을 미리 복사해 놓는 임시 장소

### 캐싱
캐싱은 애플리케이션의 처리 속도를 높여 준다. 이미 가져온 데이터나 계산된 결과값의 복사본을 저장함으로써 처리속도를 향상시키며, 이를 통해 향후 요청을 더 빠르게 처리할 수 있다. 대부분의 프로그램이 동일한 데이터나 명령어에 반복해서 액세스하기 때문에 캐싱은 효율적인 아키텍처 패턴이다.

### 웹 캐시
사용자가 웹사이트(client)에 접속할 때, 정적 컨텐츠(JS,이미지, CSS)을 특정 위치에 저장하여 웹 사이트 서버에 해당 컨텐츠를 매번 요청하여 받는 것이 아닌 특정 위치에서 불러옴으로서 사이트 응답 시간을 줄이고 서버 트래픽 감소 효과를 볼 수 있는 것을 말함  
  
**종류**  
장소에 따라 나뉨
* Browser Cashes
  * 브라우저 또는 HTTP 요청을 하는 Client Appliction에 의해 내부 디스크에 캐쉬
  * 캐시된 리소스를 공유하지 않는 한 개인에 한정된 캐시
  * 브라우저의 back버튼 또는 이미 방문한 페이지를 재 방문하는 경우 극대화
* Proxy Caches
  * Browser Cashes와 동일한 원리로 동작하며 Client나 Server가 아닌 네트워크 상에서 동작
  * 큰 회사나 IPS의 방화벽에 설치 되며 대기시간 & 트래픽 감소, 접근 정책 & 제한 우회, 사용률 기록 등을 수행
  * 한정된 수의 클라이언트를 위해 무한 대의 웹 서버 컨텐츠를 캐시
* GateWay Caches
  * 서버 앞 단에 설치되어 요청에 대한 캐시 및 효율적인 분배를 통해 가용성, 신뢰성, 성능 등을 향상
  * Encryption, SSL acceleration, Load balancing, Server / cache static content, Compression 등을 수행
  * 무한 대의 클라이언트들에게 한정된 수의 웹 서버 컨텐츠를 제공

### 어떻게 캐쉬를 컨트롤 하나
* **HTML Meta Tags**
```HTML
<META HTTP-EQUIV="EXPIRES" CONTENT="Mon, 22 Jul 2002 11:12:01 GMT">

<META HTTP-EQUIV="CACHE-CONTROL" CONTENT="NO-CACHE">
```
첫 번째 방법은 위와 같이 HTML Meta Tag를 페이지에 삽입하는 방법이다. 하지만 이 방법은 과거의 몇몇 브라우저에게만 유효 했으며 지금은 더 이상 사용하지 않는 방법이다.  

<br/>

* **HTTP Headers**

두 번째 방법은 HTTP header를 사용하는 방법으로 현재 사용하는 방식이다.  
  
파일이 이전과 비교하며 변경되었는가를 체크하는 validation과 캐쉬의 만료 여부를 체크하는 freshness로 구성. request와 response에 따라 서로 사용 될 수 있는 값이 다르며 HTTP 1.0 에서 HTTP 1.1 로 넘어오면서 약간의 변화가 있다. HTTP 1.1에서는 하위 호환 되므로 1.0의 Header를 사용하여도 정상 동작하지만 중복으로 선언된다면 1.1에 정의된 것이 우선 순위를 가지게 된다.  
  
예를 들어 Last-Modified와 Etag가 동시에 있다면 Etag가 우선순위를 가진다. Expires와 Cache-control도 마찬가지  
  
HTTP 1.1 의 Cache-Control은 하나의 값이 아니라 다양한 지시자를 이용하여 값을 전달할 수 있다. 그로 인해 여러가지 컨트롤을 가능하게 만들어준다.  
ex) `Cache-Control : max-age=3600, must-revaliate`

### 어떻게 캐쉬가 동작하나
**첫 요청**
1. 브라우저는 서버에 index.html 파일 요청
2. 서버는 index.html 파일을 찾아보고 존재하는 파일이라면 파일 내용을 브라우저에게 몇 가지 header값과 함께 응답
3. 브라우저는 응답 받은 내용을 브라우저에 표시하고 응답 헤더의 내용에 따라 캐쉬 정책을 수행
  * 만약 응답 헤더에 Last-Modified, Etag, Expires, Cache-Control:max-age 항목이 존재 한다면 복사본을 생성하고 그 값을 저장

**재 요청**
1. LAST-MODIFIED
* 브라우저는 최초 응답 시 받은 Last-Modified 값을 If-Modified-Since 라는 헤더에 포함 시켜 페이지를 요청
* 서버는 요청 파일의 수정 시간을 If-Modified-Since 값과 비교하여 동일하다면 304 NOT Modified로 응답하고 다르다면 200 OK와 함께 새로운 Last-Modified 값을 응답 헤더에 전송
2. Etag(Entitiy Tag)
* 브라우저는 최초 응답 시 받은 Etag값을 If-None-Match 라는 헤더에 포함 시켜 페이지를 요청
* 서버는 요청 파일의 Etag값을 If-None-Match값과 비교하여 동일하다면 304 Not Modified로 응답하고 다르다면 200 OK와 함께 새로운 Etag 값을 응답 헤더에 전송
* 브라우저는 응답 코드가 304인경우 캐쉬에서 페이지를 로드하고 200이라면 새로 다운받은 후 Etag값을 갱신
* Etag는 서버마다 생성하는 값이 다르며 파일마다 고유한 값을 가짐
* LAST-MODIFED(1.0) 와 ETAG(1.1) 는 validation을 체크. 이를 체크하기 위해 서버와 한번의 통신이 발생하게 되며 그로 인해 요청과 응답에서 header와 cookie등에 의한 데이터 전송(1KB)이 발생하게 됨
3. Expires
* 브라우저는 최초 응답 시 받은 Expires 시간을 비교하여 기간 내라면 서버를 거치지 않고 바로 캐쉬에 페이지를 로드. 만약 기간이 만료되었다면 위에 설명한 Validation 작업을 수행
4. Cache-Control
* 브라우저는 최초 응답 시 받은 Cache-Control 중 max-age값을 GMT와 비교하여 기간 내라면 서버를 거치지 않고 캐쉬에서 페이지를 로드. 만약 기간이 만료 되었다면 Validation 작업을 수행
* Expires(1.0)와 Cache-Control : max-age(1.1)는 freshness를 체크. 기간 내라면 서버와 통신을 하지 않고 캐쉬를 사용
* 시간은 HTTP date 형태이며 로컬 타임이 아닌 GMT를 사용
* 서버가 Last Modified Time 또는 Last Access Time을 기준으로 하여 일정 시간 이후로 Expires 또는 max-age를 설정

### 캐시 서버
웹 캐시 서버는 프록시의 한 형태라고 할 수 있다. 웹 캐시 서버는 클라이언트가 요청한 컨텐츠들을 기억하고 있다가 어느 한 클라이언트(동일사용자 일수도 있고, 다른 사용자일수도 있는)가 웹 캐시 서버가 기억하고 있는 동일한 컨텐츠를 또 다시 요청하는 경우 이를 직접 응답하여 웹 서버의 부하를 절감시켜 주는 역할을 한다.  
  
캐시서버는 일반적으로 기업 내의 인터넷 사용자와 비교적 가까이 있는 서버로서, 모든 사용자들이 자주 요청할 만한 웹페이지나, FTP 및 기타 다른 파일들을 저장하고 있다가, 이들 페이지나 파일들에 대한 이어지는 요구들을 (그때마다 인터넷에 가서 찾지 않더라도) 쉽게 만족시켜줄 수 있도록 하기 위한 서버이다. 캐시서버는 사용자들이 요구하는 정보를 더 빠르게 제공할 뿐 아니라, 인터넷의 트래픽을 획기적으로 줄여준다.

## E-TAG
HTTP response header에서 특정 버전의 리소스를 식별하는 식별자. 쉽게 말하면, 리소스가 바뀌었는지 확인하는 식별자

### 왜 사용할까
ETag는 사용하는 캐시가 유효한지 검증하기 위해 사용한다. 서버의 리소스가 변경된다면 저장해 놓은 캐시의 데이터와 서버의 리소스 데이터는 다른 값이다. 그때 캐시가 서버에게 리소스가 변경되었는지 안 되었는지 물어보는 것을 캐시 유효성 검사라고 한다. ETag를 사용하여 캐시 유효성 검사를 한다.

## 성능 테스트
특정 워크로드에서 애플리케이션의 안정성과 속도, 확장성 및 반응성이 어떻게 유지되는지를 판별하는 비기능적 소프트웨어 테스트 기법. 성능 테스트의 주요 목적은 소프트웨어 애플리케이션의 성능 병목 현상을 식별하고 제거하는 것
>워크로드 : 지정된 시간(매일 작업 중, 사용량이 가장 많은 시간, 시스템이 가장 많이 사용되는 날 등)에 시스템의 예상로드를 혼합한 것으로 정의

### 성능 테스트가 필요한 이유
조직들은 다음과 같은 이유 중 하나 때문에 성능 테스트를 함
* 애플리케이션이 성능 요건을 충족하는지 판별(예를 들어 시스템은 최대 1,000명의 동시 사용자를 처리해야 함)
* 애플리케이션 내 컴퓨팅 병목 현상을 유발하는 위치 파악
* 실제 성능 수준이 소프트웨어 벤더가 주장하는 성능 수준과 동일한지 판별
* 두 개 이상의 시스템 비교 및 가장 성능이 좋은 시스템 식별
* 최대 트래픽 이벤트에서 안정성 측정

### 성능 테스트 속성
* 속도  
소프트웨어 제품이 빠르게 반응하는지 여부를 결정
* 확장성  
소프트웨어 제품이 한 번에 처리 할 수 있는 로드 양을 결정
* 안정성  
다양한 워크로드의 경우 소프트웨어 제품이 안정적인지 여부를 결정
* 신뢰성  
소프트웨어 제품이 안전한지 여부를 결정

### 성능 테스트의 목적
* 성능 테스트의 목적은 성능 정체를 제거하는 것
* 제품이 시장에 출시되기 전에 개선해야 할 사항을 파악
* 성능 테스트의 목적은 소프트웨어를 신속하게 만드는 것
* 성능 테스트의 목적은 소프트웨어를 안정적이고 신뢰할 수 있도록 만드는 것

### Virtual User
주로 퍼포먼스 테스트를 위해 만드는 가상 유저

### TPS(RPS)
Transaction Per Second 의 약자로 주로 서버 성능의 척도가 됨. 초당 트랜잭션 처리수 를 의미  
  
일반적으로 Active User의 수에 비례하고 유저들에 대한 평균 응답시간(MRTT)에 반비례  
  
![image](https://user-images.githubusercontent.com/44667299/163721008-cdda4766-c4db-42c4-8295-1741fac9cbfa.png)  
사진 출처 : https://ch4njun.tistory.com/266  
  
위 그래프에서 보듯이 서비스를 이용하는 사용자가 증가하면 TPS는 지속적으로 늘어난다. 하지만 어느 시점이 되면 아무리 사용자가 늘어나더라도 TPS는 더 이상 늘어나지 않게 된다. 이러한 지점을 Saturation Point(포화지점)이라고 한다.  
  
이러한 상황은 잘못된 것이 아니라 오히려 이상적인 상황이다. 제대로 튜닝이 되지 않은 서비스에서는 포화지점을 지나면 오히려 TPS 가 떨어지기도 한다.  
  
그러면 포화지점에서 더 이상 TPS가 증가하지 않는 것은 무슨 의미를 가질까?  
  
초당 처리할 수 있는 Transaction 의 수가 한계에 도달했고 그때부터 사용자가 증가하면 Latency 시간이 증가한다.   
  
이러한 포화지점이 중요한 이유는 여기를 기준으로 해당 서버가 감당할 수 있는 부하의 한계를 정의할 수 있기 때문이다.

### MAU/DAU
**MAU(Monthly Active Users)**
* 월별 활동한 이용자를 의미. 한 달에 몇 명이나 이 서비스를 이용하는지에 대해 구분할 때에 사용
* 꽤 정착되어 있는 디지털 플랫폼 유저의 충성도를 확인하기 위한 지표로 활용
* 잘 정착된 서비스는 MAU가 비교적 꾸준하게 유지됨
  
**DAU(Daily Active Users)**
* 일별 활동 사용자 수를 의미. 하루 동안 방문한 사용자가 총 몇 명인지 말해주는 지표
* 유저의 특정 이용 패턴을 분석하는데에도 적절하고, 단기적인 이벤트의 호응도를 알아내는데도 탁월
* 이 수치를 바탕으로 서버의 부하나 트래픽을 예측하여, 인프라를 최적화할 수도 있음
* 실시간 이슈 발생 시 가장 영향을 많이 받는 지표

**Stickness**
* DAU / MAU
* 매달 혹은 매주 사용하는 유저중 얼마나 많은 유저가 매일 사용하는지를 계산한 지표
* 서비스가 얼마나 활성화 되어있는지 확인이 가능함 ( 활성도 혹은 매력도 )
* 예를 들어 Stickness 가 낮다는건 광고나 그런걸로 사람들이 호기심을 가지고 서비스에 접근했지만 막상 써보니 구려서 떠나버린 서비스를 의미하니깐, 이런 경우에는 광고보다는 서비스 품질을 높이는데 주력해야 함

# 기술

## 로깅 라이브러리 비교(스프링/자바 -> 보안 이슈 적지 말아라)

### 로깅
운영 중인 웹 어플리케이션이 문제가 발생했을 경우, 문제의 원인을 파악하려면 문제가 발생했을 때 당시의 정보가 필요. 이런 정보를 얻기 위해서 Exception이 발생했거나, 중요 기능이 실행되는 부분에서는 적절한 로그를 남겨야 함
* 정보를 제공하는 일련의 기록인 로그(log)를 생성하도록 시스템을 작성하는 활동
* 프린트 줄 넣기(printlning)는 간단한, 보통은 일시적인, 로그를 생성하기만 함
* 시스템 설계자들은 시스템의 복잡성 때문에 로그를 이해하고 사용해야 함
* 로그가 제공하는 정보의 양은, 이상적으로는 프로그램이 실행되는 중에도 설정 가능해야 함
* 일반적인 로그 기록의 이점
  * 로그는 재현하기 힘든 버그에 대한 유용한 정보를 제공할 수 있다.
  * 로그는 성능에 관한 통계와 정보를 제공할 수 있다.
  * 설정이 가능할 때, 로그는 예기치 못한 특정 문제들을 디버그하기 위해, 그 문제들을 처리하도록 코드를 수정하여 다시 적용(deploy)하지 않아도, 일반적인 정보를 갈무리할 수 있게 한다.

### Log Level
* trace : debug보다 세분화된 정보
* debug : 디버깅하는데 유용한 세분화된 정보
* info : 진행상황 같은 일반 정보
* warn : 오류는 아니지만 잠재적인 오류 원인이 될 수 있는 경고성 정보
* error : 요청을 처리하는 중 문제가 발생한 오류 정보

### 로그 라이브러리 종류
* java.util.logging
  * JDK 1.4부터 포함된 표준 로깅 API
  * 별도 라이브러리 추가 불필요
  * 기능이 많이 부족해 다른 로그 라이브러리를 많이 사용
* Apache commons logging
  * 아파치 재단의 Commons 라이브러리 중에 로그 출력을 제공하는 라이브러리
* log4j
  * 아파치 재단에서 제공하며 가장 많이 사용되는 로깅 라이브러리
  * 현재는 개발이 중단되었기에 새 프로젝트에 적용하려면 다른 로깅 프레임워크를 사용해야 함
* slf4J
  * 로깅 퍼사드(facade)로서, log4j, logback, commons-logging과 같은 로깅 프레임워크를 위한 추상화를 제공
  * 스프링 부트에서 이것들을 통합해서 **인터페이스**로 제공하는 것
  * 예를 들어 log4j를 사용하다가 log4j2로 로깅 프레임워크를 교체하면 많은 코드 수정이 발생한다. 이런 점을 고려하면 slf4j를 사용하고, log4j를 연결하여 사용하는 것이 바람직
* logback
  * Log4j의 단점 개선 및 기능을 추가하여 개발한 로깅 라이브러리
  * 구현체
* log4j2
  * 가장 최근에 등장
  * logback과 동일하게 자동 리로드 기능과 필터링 기능을 제공

단순하게 비교만 해본다면 가장 최신이자 빠르며 logback의 아키텍처에서 발생하는 문제점을 수정한 이라고 Appache가 소개하는 log4j2를 권장.  
Apache에 따르면 멀티 스레드 환경에서의 비동기 로거(Async Logger)의 경우 log4j 1.x 및 logback보다 몇 배는 빠른 처리량을 보인다고 한다. 그리고 람다 표현식과 사용자 정의 로그 레벨도 지원해줌  
slf4j + log4j

## RabbitMq
* 메세징 큐 시스템
* 얼랭(Erlang)으로 AMQP를 구현한 메시지 브로커 시스템
>얼랭(Erlang) : 범용 병렬 프로그래밍 언어  
>AMQP(Advanced Message Queuing Protocol) : 메시지 지향 미들웨어를 위한 개방형 표준 응용 계층 프로토콜

### 아키텍쳐
![image](https://user-images.githubusercontent.com/44667299/163817756-5a2460e9-6ba4-4439-9f8f-beb3125d959b.png)  
사진 출처 : https://blog.dudaji.com/general/2020/05/25/rabbitmq.html

* Message
  * 처리해야할 내용이 담겨져 있음
* Producer
  * 메세지를 생성하고 발송하는 주체
  * message를 consumer에게 위임하기 위해 message를 exchange에 publish 하는 자
  * message를 보내는 일 이외에는 아무일도 하지 않음
  * 메세지는 Queue에 저장이 되는데 Producer는 Queue에 직접 접근하지 않고 항상 Exchange를 통해 접근
* Consumer
  * message를 Producer로 부터 위임 받아 처리하는 자
  * 메세지를 수신하는 주체
  * Queue에 직접 접근하여 메세지를 가져옴
* Queue
  * Producer들이 발송한 메세지들이 Consumer가 소비하기 전까지 보관되는 장소
* Exchange
  * Producer에서 전달받은 message를 Queue에게 전달 해줌
  * Producer들에게서 전달받은 메세지들을 어떤 Queue들에게 발송할지를 결정
  * 메시지를 어떤 Queue에 추가할지, 얼마나 추가할지, 아니면 그냥 버려야할지 이는 Exchange 규칙에 의해 결정
  * 네 가지 타입이 있으며, 일종의 라우터 개념
* Bindings
  * 생성된 Exchange 에는 전달받은 메시지를 원하는 Queue로 전달하기 위해 Bindings이라는 규칙을 정의할 수 있다.
  * 간단하게 목적지 Queue이름만으로도 Binding을 추가할 수 있고, 일부 Exchange type에 따라 routing key를 지정해서 메시지를 필터링 한 후 지정한 Queue로 보내도록 정의할 수 있다.

Exchange 타입
|타입|설명|특징|
|---|---|---|
|Direct|Routing key가 정확히 일치하는 Queue에 메세지 전송|Unicast|
|Topic|Routing key 패턴이 일치하는 Queue에 메세지 전송|Multicast|
|Headers|[key:value]로 이루어진 header 값을 기준으로 일치하는 Queue에 메세지 전송|Multicast|
|Fanout|해당 Exchange에 등록된 모든 Queue에 메세지 전송|Broadcast|

### 왜 사용하나
**Microservices 간의 연결**  
Microservice들 간 연결로 사용할 여러 서비스
* Brokers (ex : RabbitMQ / Kafka)
* Remote Prodecure Calls (RPC)
* REST APIS

<br/>

**Message Queueing**
* Message Queues를 사용하면 응용 프로그램의 일부가 메시지를 대기열에 비동기식으로 푸시하고 올바른 대상으로 전달되는지 확인할 수 있음
* 메시지 브로커는 수신 서비스가 사용 중이거나 연결이 끊어졌을 때 임시 메시지 저장소를 제공함

<br/>

**Microservices 아키텍처에서 브로커 역할**  
RabbitMQ는 비동기 처리를 가능하게 한다. 즉, 메시지를 즉시 처리하지 않고 큐에 넣을 수 있다.  
  
따라서 RabbitMQ는 장기 실행 작업(long-running tasks) 또는 차단 작업(blocking tasks)에 이상적이며, 웹 서버가 현장에서 계산 집약적인 작업(computationally intensive tasks)을 수행하지 않고 요청에 신속하게 응답 할 수 있다.  
  
RabbitMQ는 단순히 메시지를 저장하고 준비가 되면 Consumers에게 전달한다.

* RabbitMQ는 신뢰할 수있는 오픈 소스 메시지 브로커이다.
  * 지속적으로 업데이트되고 개선되고 있음
* RabbitMQ는 기본적으로 AMQP 0.9.1을 구현하는 AMQP, MQTT, STOMP 등과 같은 여러 표준화 된 프로토콜을 지원한다.
  * 다양한 표준화 된 메시지 프로토콜을 지원하는 RabbitMQ의 기능은 다양한 시나리오에서 사용할 수 있으며 RabbitMQ 브로커를 AMQP 기반 브로커로 대체 할 수 있음을 의미
* RabbitMQ는 다양한 산업 분야의 많은 회사에서 사용하고 있으며 대기업(Zalando, WeWork, Wunderlist, Bloomberg 등)에서 사용하고 신뢰한다.
  * Microservices 기반 아키텍처에 의존
* RabbitMQ는 사용자 친화적이며 이러한 RabbitMQ 모범 사례를 따르면 의도 한 목적에 맞게 구성을 쉽게 조정할 수 있다.
  * RabbitMQ는 Erlang으로 작성되었으며 세계에서 가장 많이 배포 된 오픈 소스 메시지 브로커이다.
  * 즉, 잘 테스트되고 강력한 브로커임
* RabbitMQ 브로커는 확장 가능하고 유연하다.
  * 팀은 대기열과 메시지를주고받는 Producer와 Consumer만 유지하면 됨
  * 부하가 높을 때 대기열이 커지면 Consumer를 더 추가하고 작업을 병렬화하는 것이 표준
  * 이것은 간단하고 효과적인 확장 방법

<br/>

RabbitMQ는 다재다능하고 신뢰할 수 있는 메시지 브로커임

## ActiveMq
* JMS(Java Message Service) 클라이언트와 함께 자바로 작성된 오픈 소스 메시지 브로커
* 하나 이상의 클라이언트나 서버로부터 통신을 조성시키는 엔터프라이즈 기능들을 제공
* 자바 및 기타 여러 언어 간 클라이언트 지원
* Apache ActiveMQ 는 가장 대중적이고 강력한 오픈 소스 메세징 그리고 통합 패턴 서버다.
* ActiveMQ는 JMS를 지원하는 클라이언트를 포함하는 브로커, 자바 뿐만 아니라 다양한 언어를 이용하는 시스템 간의 통신을 할 수 있게해준다. 또한 클러스터링기능 및 DB 그리고 FileSystem을 통해 각 시스템간의 일관성 및 지속성을 유지 시켜준다.
* ActiveMQ는 자바로 만든 오픈소스 메세지 브로커이다. JMS 1.1을 통해 자바 뿐만 아니라 다른 언어를 사용하는 클라이언트를 지원한다.
* Apache ActiveMQ는 빠르며, 다양한 언어간의 클라이언트 및 프로토콜을 지원하고, 사용하기 쉬운 엔터프라이즈 통합 패턴 및 많은 고급 기능을 제공하면서 JMS 1.1 및 J2EE 1.4를 완벽하게 지원한다.
* MOM(메시지 지향 미들웨어) 이다.
* 간단히 정의하면 클라이언트 간 메시지를 송수신 할 수 있는 오픈 소스 Broker(JMS 서버)다.

### JMS
* JMS 는 자바 기반의 MOM(메시지 지향 미들웨어) API 이며 둘 이상의 클라이언트 간의 메시지를 보낸다.
* JMS 는 자바 플랫폼, 엔터프라이즈 에디션(EE) 기반이며, 메시지 생성, 송수신, 읽기를 수행하며 비동기적이며 신뢰할 만하고 느슨하게 연결된 서로 다른 분산 어플리케이션 컴포넌트 간의 통신을 허용한다.
* JMS의 핵심 개념은 Message Broker와 Destination이다.
  * Message Broker : 목적지에 안전하게 메시지를 건네주는 중개자 역할
  * Destination : 목적지에 배달될 2가지 메시지 모델 QUEUE, TOPIC
  * Queue : 메세지를 받기 위해 Consumer간 경쟁, 연결된 순서대로 메세지 제공
  * Topic : Pub/Sub 모델, 구독자 모두에게 메세지를 제공

<br/>

**JMS 메세지 구조**
* 헤더, 등록정보, 본문 3가지 부문으로 구성
* 헤더
  * JMS 메세지 필수 값, 메세지 경로 지정 및 식별에 사용되는 값 포함
* 등록정보
  * 등록정보 이름, 등록정보 값의 쌍으로 지정
  * 데이터를 작성한 프로세스에 대한 정보, 데이터가 작성된 시간, 데이터 각 부분의 구조 포함 가능
* 본문 유형

|유형|설명|
|---|---|
|StreamMessage|본문이 Java 프리미티브 값의 스트림을 포함하는 메시지. 이 메시지는 순차적으로 채워지고 읽혀진다.|
|MapMessage|본문에 일련의 이름-값 쌍을 포함하는 메시지. 항목 순서는 정의되지 않는다.|
|TextMessage|본문에 Java 문자열을 포함하는 메시지. 예 : XML 메시지|
|ObjectMessage|본문에 일련화된 Java 객체를 포함하는 메시지|
|BytesMessage|본문에 해석되지 않은 바이트의 스트림이 포함된 메시지|

<br/>

**JMS API 구현 순서**  
ConnectionFactory → Connection → Session → MessageProducer → send

### ActiveMQ 메세지 처리 구조
![image](https://user-images.githubusercontent.com/44667299/163977827-939583e2-aec8-4833-9514-924d68bf436c.png)
* 기본적으로 Message를 생상하는 Producer, ActiveMQ Broker(Server), Message를 소비하는 Consumer로 구성되어 있음

![image](https://user-images.githubusercontent.com/44667299/163977973-83012c35-7b8f-4efb-9172-3b147a055216.png)
* QUEUE 모델의 경우 메세지를 받는 Consumer가 다수일 때 연결된 순서로 메세지는 제공된다.
* TOPIC 모델의 경우 메세지를 받는 Consumer가 다수일 때 메세지는 모두에게 제공된다.

### ActiveMQ의 장점
* 분리
  * 대기열은 시스템 사이에 있으며, 하나의 시스템 장애는 다른 대기열에 영향을 주지 않는다. 메세지 통신은 대기열을 통해 이루어진다.
  * 시스템이 가동 중일때도 계속 작동한다.
  * (클라이언트와 서버간의 연결과 QUEUE 대기열의 역할이 분리)
* 복구 지원
  * QUEUE의 처리가 실패하면 나중에 메세지를 복원할 수 있다.
* 신뢰성
  * 클라이언트 요청을 처리하는 시스템을 생각해보자. 정상적인 경우 시스템은 분당 100건의 요청을 받고 이 시스템 요청 수가 평균을 넘어서는 경우 신뢰할 수 없다. 이 경우 QUEUE는 요청을 관리할 수 있으며 시스템 처리량을 기초로 주기적으로 메세지를 전달할 수 있다.
  * QUEUE에서 들어온 메세지에 대한 처리를 관리하기 때문에 신뢰할 수 있는 시스템
* 비동기식 처리
  * 클라이언트와 서버 통신이 비 차단되어있다.
  * 클라이언트가 서버에 요청을 보내면 응답을 기다리지 않고 다른 작업을 수행할 수 있다. 응답을 받으면 클라이언트는 언제든지 처리할 수 있다.
* 다양한 언어와 프로토콜 지원 (Java, C, C++, C#, Ruby, Perl, Python, PHP 클라이언트 지원)
* OpenWire를 통해 고성능의 Java, C, C++, C# 클라이언트 지원
* Stomp를 통해 C, Ruby, Perl, Python, PHP 클라이언트가 다른 인기있는 메시지 브로커들과 마찬가지로 ActiveMQ에 접근 가능
* Message Groups, Virtual Destinations, Wildcards와 Composite Destination를 지원
* Spring 지원으로 ActiveMQ는 Spring Application에 매우 쉽게 임베딩될 수 있으며, Spring의 XML 설정 메커니즘에 의해 쉽게 설정 가능
* Geronimo, JBoss 4, GlassFish, WebLogic과 같은 인기있는 J2EE 서버들과 함께 테스트됨
* 고성능의 저널을 사용할 때에 JDBC를 사용하여 매우 빠른 Persistence를 지원
* REST API를 통해 웹기반 메시징 API를 지원
* 웹 브라우저가 메시징 도구가 될 수 있도록, Ajax를 통해 순수한 DHTML을 사용한 웹 스트리밍 지원

참고 : https://velog.io/@rlaghwns1995/Apache-ActiveMQ%EC%9D%98-%EA%B8%B0%EB%B3%B8

## 카프카
대용량, 대규모 메시지 데이터를 빠르게 처리하도록 개발된 분산 메시징 플랫폼

### 카프카는 왜 사용하나
소스 어플리케이션 : 데이터전송, 타겟 애플리케이션 : 데이터받음 → 단방향이였지만 소스/타겟들이 점점 많아짐
* 실시간 트랜잭션(OLTP) 처리와 비동기 처리가 동시에 이루어지지만 통합된 전송 영역의 부재로 복잡도가 증가
* 파이프라인 관리가 어려움. 특정 부분을 수정해야할 때, 앞단부터 다 수정해야 할 수 있음

![image](https://user-images.githubusercontent.com/44667299/163992850-639c3cb5-c919-4857-8fc0-85a06b7d84ed.png)  
사진 출처 : https://needjarvis.tistory.com/599  
  
그러니까 카프카가 발신자와 수신자를 연결해준다.
* 발신자(Publish) : 카프카에게 데이터를 전송하기만 하면 됨. 누가 받을지는 신경안씀
* 수신자(Subscribe) : 수신자는 카프카에 원하는 토픽을 구독

그래서 누가 보냈는지 신경 안쓰고, 필요한 메세지만 구독하는 방식  
이전 point to point 구조에 비해 매우 단순해지고 유지보수, 에러발생, 네트워크 트래픽의 장점까지 얻음

**카프카 도입 전(Point to Point 방식)**  
Point to Point 방식은 큐를 통해서 전달할 메세지를 전달하면 받는 사람이 큐에서 메세지를 사용  
FIFO 큐처럼 받는 사람들은 모두 하나씩 큐를 가지고 있고, 보내는 사람들이 목적지 큐에 전달하면 받는 사람이 꺼내읽음  
→ 데이터연동의 복잡성이 증가  
![image](https://user-images.githubusercontent.com/44667299/163993477-228343d3-5f8f-4303-8cbd-48970f8a2946.png)  
사진 출처 : https://freedeveloper.tistory.com/350  
  
기존엔 데이터 스토어 백엔드에 따라 포맷, 별도의 앱개발이 필요했음  
**즉, 데이터 전송라인이 많아짐 → 배포/장애대응 어려워짐 → 데이터 포맷의 변경사항이 생길 때 유지보수 어려워짐**  

<br/>

**카프카 개발 전 링크드인의 데이터 처리 시스템**  
![image](https://user-images.githubusercontent.com/44667299/163994194-ca945c68-9ffc-459b-bae7-e909c1953e59.png)
* 기존 링크드인의 데이터 처리 시스템은 각 파이프라인이 파편화되고 시스템 복잡도가 높아 새로운 시스템을 확장하기 어려운 상황
* 기존 메시징 큐 시스템인 ActiveMQ를 사용했지만, 링크드인의 수많은 트래픽과 데이터를 처리하기에는 한계가 있었음
* 이로 인해 새로운 시스템의 개발 필요성이 높아졌고, 링크드인의 몇몇 개발자가 다음과 같은 목표를 가지고 새로운 시스템을 개발
  * 프로듀서와 컨슈머의 분리
  * 메시징 시스템과 같이 영구 메시지 데이터를 여러 컨슈머에게 허용
  * 높은 처리량을 위한 메시지 최적화
  * 데이터가 증가함에 따라 스케일아웃이 가능한 시스템

<br/>

**카프카 도입 후(Pub/Sub 방식)**  
중간에 Topic이 있다.  
구독자가 특정 토픽이나 이벤트에 구독을 해 놓으면 해당 토픽이나 이벤트에 대한 통지를 비동기 방식으로 받음.  
즉, Publisher가 Topic에 메세지를 보내면 해당 topic을 구독해놓은 모든 사용자들에게 메세지가 전송됨  
→ 프로듀서와 컨슈머를 분리해서 높은 처리량 가능, 스케일 아웃 가능  
  
![image](https://user-images.githubusercontent.com/44667299/163994795-10b84213-f159-43f0-bcec-9c8472a4b869.png)  
  
* 차이점
  * Point to Point 방식은 메세지를 각각 1명의 사용자만 받을 수 있지만,  
  Pub/Sub 방식은 토픽을 구독한 많은 사용자가 메세지를 받을 수 있다.
  * Point to Point 방식은 메세지를 받는 사람이 큐에서 꺼내읽는 방식이지만,  
  Pub/Sub 방식은 토픽에서 꺼내오는게 아니라 토픽에서 BroadCast 되는 방식. 사용자들에게 메세지가 통보되는 개념
* Pub/Sub 방식 장점
  * 카프카에만 데이터를 전달하면 필요한 곳에서 각자 가져갈 수 있음
  * 카프카가 제공해주는 표준 포맷으로 연결되서 데이터를 주고 받는게 부담이 덜해짐

<br/>

**카프카를 개발 후 링크드인의 데이터 처리 시스템**  
![image](https://user-images.githubusercontent.com/44667299/163995349-c670478a-5a68-4fd4-815e-b447ce3c42d2.png)
* 카프카를 적용함으로써 모든 이벤트/데이터의 흐름을 중앙에서 관리할 수 있게 되었으며
* 서비스 아키텍처가 기존에 비해 관리하기 심플해졌음

### 메시징 시스템
* Publisher와 Subscriber로 이루어진 비동기 메시징 전송 방식
* 메시지라고 불리는 데이터 단위를 보내는 측(Publisher/Producer)에서 메시지 시스템에 메시지를 저장하면 가져가는 측(Subscriber/Consumer)는 데이터를 수신
* 발신자의 메시지에는 수신자가 정해져 있지 않은 상태로 발행(Publish)
* 구독(Subscribe)을 신청한 수신자만이 정해진 메시지를 받음
* 수신자는 발신자 정보가 없어도 메시지 수신이 가능

<br/>

**일반적인 형태의 네트워크 통신의 문제점**
* N:N연결이므로 클라이언트 갑자기 많아질 경우 느려질 수 있고 이를 대응하기 위한 확장성이 떨어짐
* 특정 클라이언트가 다운되서 메시지를 받지 못하게 될 경우 메시지가 유실될 수 있음

<br/>

**Pub/Sub 모델**
* 작동방식
  * 프로듀서가 메시지를 컨슈머에게 직접 전달하는 것이 아니라, 중간의 메시징 시스템에 전달 (수신처 ID포함)
  * 메시징 시스템의 교환기가 메시지의 수신처 ID값을 통해 컨슈머들의 큐에 메시지를 전달 (push)
  * 컨슈머는 큐를 모니터링하다가 큐에 메시지가 있을 경우 값을 회수
* 장점
  * 특정 개체가 수신불능 상태가 되더라도 메시징 시스템만 살아있다면 메시지가 유실되지 않음
  * 메시징 시스템으로 연결되어있기 때문에 확장성이 용이
* 단점
  * 직접 통신하지 않기 때문에 메시지가 잘 전달되었는지 파악하기 힘듬
  * 중간에 메시징 시스템을 거치기 때문에 메시지 전달 속도가 빠르지 않음

### 카프카
![image](https://user-images.githubusercontent.com/44667299/163996108-9e067023-fab1-416f-878b-2f76044f8d8a.png)  
  
**카프카 아키텍처**
* 프로듀서(Producer)
  * 메시지를 생산하여 브로커의 토픽으로 전달하는 역할
* 브로커(Broker)
  * 카프카 애플리케이션이 설치되어 있는 서버 또는 노드를 지칭
* 컨슈머(Consumer)
  * 브로커의 토픽으로부터 저장된 메시지를 전달받는 역할
* 주키퍼(Zookeeper)
  * 분산 애플리케이션 관리를 위한 코디네이션 시스템
  * 분산된 노드의 정보를 중앙에 집중하고 구성관리, 그룹 네이밍, 동기화 등의 서비스 수행

<br/>

**작동방식**
* 프로듀서는 새 메시지를 카프카에 전달
* 전달된 메시지는 브로커의 토픽이라는 메시지 구분자에 저장
* 컨슈머는 구독한 토픽에 접근하여 메시지를 가져옴 (pull 방식)

<br/>

**기존 메시징 시스템과 다른 점**
* 디스크에 메시지 저장
  * 기존 메시징 시스템과 가장 큰 특징
  * 기존 메시징 시스템은 컨슈머가 메시지를 소비하면 큐에서 바로 메시지를 삭제
  * 하지만, 카프카는 컨슈머가 메시지를 소비하더라도 디스크에 메시지를 일정기간 보관하기 때문에 메시지의 손실이 없음 (영속성)
* 멀티 프로듀서, 멀티 컨슈머
  * 카프카의 경우 디스크에 메시지를 저장하는 특징으로 인해, 프로듀서와 컨슈머 모두 하나 이상의 메시지를 주고 받을 수 있음
* 분산형 스트리밍 플랫폼
  * 단일 시스템 대비 성능이 우수하며, 시스템 확장이 용이함
  * 일부 노드가 죽더라도 다른 노드가 해당 일을 지속 (고가용성)
* 페이지 캐시
  * 카프카는 잔여 메모리를 이용해 디스크 Read/Write 를 하지 않고 페이지 캐시를 통한 Read/Write으로 인해 처리속도가 매우 빠름  
  ![image](https://user-images.githubusercontent.com/44667299/163996573-5c9a80d0-a21e-4b45-b8ca-c99933a4010c.png)
* 배치 전송 처리
  * 서버와 클라이언트 사이에서 빈번하게 발생하는 메시지 통신을 하나씩 처리할 경우 그만큼 네트워크 왕복의 오버헤드가 발생
  * 이로 인해, 메시지를 작은 단위로 묶어 배치 처리를 함으로써 속도 향상에 큰 도움을 줌

### 카프카 데이터 모델
![image](https://user-images.githubusercontent.com/44667299/163997059-1f9047eb-b569-46bf-af80-c16dafecc2b0.png)  
카프카는 기본적으로 토픽과 파티션이라는 데이터 모델이 존재  

<br/>

**토픽(Topic)**
* 메시지를 논리적으로 묶은 개념 (데이터베이스의 테이블 / 파일시스템의 폴더와 유사한 개념)
* 프로듀서가 메시지를 보낼경우 토픽에 메시지가 저장됨

![image](https://user-images.githubusercontent.com/44667299/163998415-642f75af-baf4-4bdc-b213-d5e501be05e9.png)  

<br/>

**파티션(Partition)**
* 토픽을 구성하는 데이터 저장소이며 메시지가 저장되는 위치  
  ![image](https://user-images.githubusercontent.com/44667299/163998599-f28b2199-0cdc-40d0-b3f4-125a88a47eb2.png)
* 여러개의 프로듀서에서 한개의 파티션으로 메시지를 보낼 경우 병목이 생기고, 메시지의 순서를 보장할 수 없게 됨
* 그렇기에, 파티션을 여러개로 늘리고 그 수만큼 프로듀서도 늘려 하나의 파티션마다 하나의 프로듀서 메시지를 받으면 훨씬 빠름
* 그렇다면 파티션은 많을 수록 좋은가?
  * 파일 핸들러의 낭비가 존재
    * 각 파티션은 브로커의 디렉토리와 매핑되고 저장되는 데이터마다 2개의 파일(인덱스, 실제 파일)이 있기 때문에 너무 많은 파일 핸들이 생길 경우 리소스 낭비가 생김
  * 장애 복구 시간이 증가할 수 있음
    * 카프카는 리플리케이션(Replication)을 지원하고, 이를 통해 지속적으로 리더 파티션을 팔로워 파티션으로 리플리케이션을 하게됨
    * 하지만 파티션 수가 너무 많을 경우 리플리케이션 수행이 느려져 장애복구시간이 증가할 수 있음
  * 추후 파티션 수를 줄이는 것이 불가능
    * 카프카에서 파티션 수를 늘리는 것은 아무때나 가능하지만 파티션 수를 줄이는 방법은 제공하지 않음. 만약 줄이고 싶다면 토픽 자체를 삭제하는 것 말고는 방법이 없음

<br/>

**오프셋과 메시지순서**
* 오프셋(offset)  
  ![image](https://user-images.githubusercontent.com/44667299/163999243-234d35b1-7118-469f-b939-45a32f25f406.png)
  * 파티션마다 메시지가 저장되는 위치
  * 파티션 내에서 순차적으로 유니크하게 증가하는 숫자 형태로서 동일 파티션 내 메시지의 순서를 보장해줌
  * 컨슈머는 메시지를 가져올 때마다 오프셋 정보를 커밋(commit)함으로써 기존에 어디 위치까지 가져왔는지 알 수 있게 됨

<br/>

**파티션과 메시지순서**
* 파티션을 여러 개 지정할 경우
  * 프로슈머가 메시지를 보낼 때, 파티션이 여러 개인 경우 메시지는 각각의 파티션으로 순차적으로 배분 (Round-robin 방식)
  * 메시지의 순서는 오로지 동일 파티션 내 오프셋을 기준으로만 보장되기 때문에,
  * 여러 개의 파티션을 사용할 경우 동일 파티션 내에서는 순서가 보장되지만, 파티션과 파티션 사이에서는 순서를 보장하지 못하기 때문에 전체 메시지를 출력할 경우 순서가 섞일 수 있음
  * 전체 메시지의 순서를 보장하고 싶은 경우 partition을 1개로만 설정해야 함
    * 하지만 파티션이 하나이므로 분산 처리가 불가능

<br/>

**리플리케이션**
* 고가용성 및 데이터 유실을 막기 위해 replication을 수행
* 원본 파티션의 경우 '리더'가 되고, 복제 파티션의 경우 '팔로워'가 됨
* 만약 리더 파티션이 있는 브로커가 다운될 경우, 복제 파티션을 가진 브로커의 팔로워 파티션이 새로운 리더가 되어 정상적으로 프로듀서의 요청을 처리

<br/>

**ISR(In Sync Replica)**
* 리더와 팔로워로 이루어진 리플리케이션 그룹
* 리플리케이션 그룹 내 동기화 및 신뢰성 유지
* 팔로워는 Read/Write 권한이 없고 오로지 리더로부터 데이터를 복제하기 때문에 특정 팔로워가 다운되서 리플리케이션을 못할 경우 동기화 문제 발생
* 이러한 문제로 인해, 문제가 감지된 팔로워(설정된 일정주기(replica.lag.time.max.ms)만큼 요청이 오지 않는 팔로워)는 ISR 그룹에서 추방됨

![image](https://user-images.githubusercontent.com/44667299/163999811-eaed351d-c1fe-4366-8715-41181171697d.png)  

<br/>

**Consumer 그룹**
* 컨슈머가 메시지를 소비하는 시간보다 프로듀서가 메시지를 전달하는 속도가 더 빨라서 메시지가 점점 쌓일 경우를 대비하여 동일 토픽에 대해 여러 컨슈머가 메시지를 가져갈 수 있도록 컨슈머 그룹이라는 기능을 제공
* 아래와 같이 하나의 consumer가 프로듀서의 메시지 전송 속도를 따라가지 못할 경우  
  ![image](https://user-images.githubusercontent.com/44667299/164000109-14e65180-ab93-43b0-acee-6a58b9c2787d.png)
* 아래와 같이 컨슈머를 확장하여 하나의 파티션 당 하나의 컨슈머가 연결되도록 할 수 있음 (리밸런스, rebalance)  
  ![image](https://user-images.githubusercontent.com/44667299/164000155-36226ea4-7ab7-402e-a402-b09c898239fe.png)
* 하나의 파티션 당 하나의 컨슈머가 1:1 연결되어야 하므로 아래와 같이 확장할 수는 없음  
  ![image](https://user-images.githubusercontent.com/44667299/164000201-335552f6-d4e9-4612-8497-b7b9877ffefb.png)
* 컨슈머 그룹은 컨슈머가 일정한 주기로 하는 하트비트(컨슈머가 poll()하거나 메시지의 오프셋을 커밋할때 보냄)를 통해 컨슈머가 메시지를 처리하고 있다는 것을 인지하며, 만약 오랫동안 하트비트가 없다면 해당 컨슈머의 세션이 타임아웃되고 리밸런스를 수행
* 카프카의 메시지 큐 시스템은 큐에서 메시지를 가져가도 사라지지 않기 때문에 여러 컨슈머 그룹이 동일 토픽에 붙을 수 있음

### 카프카는 어떤 경우에 필요한가
**Kafka vs RabbitMQ vs Google Cloud Pub/Sub**
* 모두 비동기 통신을 제공하며, 프로듀서와 컨슈머가 분리되어 있음

<br/>

**RabbitMQ**
* AMQP(Advanced Message Queuing Protocol, 클라이언트와 미들웨어간 메시지 교환 개방형 표준 프로토콜)을 위해 개발 되어 다른 AMQP 프로토콜 기반 MQ(ex. RabbitMQ, ActiveMQ, ZeroMQ)등과 데이터 교환이 수월
* 필요에 따라 동기/비동기 구현 가능
* 유연한 라우팅이 가능(exchanger가 메시지를 적절히 각각의 queue에 분배)하여 관리가 쉽고, 관리UI가 직관적이고 편리
* broker 중심적이며, producer와 consumer간 메시지 전달 보장에 초점을 맞추어 신뢰성이 높음
* 20k+/sec 처리 보장

<br/>

**Kafka**
* 고성능, 고가용성, 확장성
* 분산 처리 시스템으로서, 확장성(scalability) 및 고가용성(high available)이 높음
* 따라서, 노드 장애에 대한 대응성이 높음
* 배치 처리가 가능해 네트워크 왕복 오버헤드를 줄일 수 있음
* 디스크 파일 시스템에 데이터를 저장함으로써 영속성(persistency)을 보장 (즉, 오류 시 복구가 가능)
* Producer 중심적이며, 메시지 전달 보장이 optional
  * 메시지 전달 보장을 할 경우 처리속도가 느려져(리더와 팔로워 모두에게 ack(응답 승인)을 받야아 하므로) 카프카의 처리속도 측면의 장점이 상쇄
* 라우팅 기능이 없음 (producer가 직접 적절한 topic과 partion으로 보내줘야 함)
* 100k+/sec 처리 보장

<br/>

**Kafka**
* 높은 처리량 및 고성능/분산/스케일 아웃이 중요한 경우
* 가용성(장애 대응)이 높아야 하는 경우
* 메시지 전달 보장이 필수적이지 않은 경우
* 메시지 처리 순서가 보장되어야 하는 경우
* 스트리밍 데이터 처리가 필요한 경우
* 메시지 영속성이 필요한 경우

<br/>

**RabbitMQ**
* 빠르고 쉽게 메시지 큐 시스템을 구축하고자 하는 경우 (라우팅 기능이 유연함)
* 메시지 전달 보장이 필수적인 경우
* 메시치 처리 순서가 보장되지 않아도 되는 경우
* 메시지 영속성 X

<br/>

**Google Pub/Sub**
* Kafka와 유사한 아키텍처 (topic 개념)
* 클라우드 서비스로서 별도의 설치나 운영이 필요없음
* 메시지 처리 순서 보장이 안됨(분산형 아키텍처)
* 메시지 영속성 X  
  (저장기간 최대 7일)

참고 및 출처 : https://pearlluck.tistory.com/288, https://velog.io/@jaehyeong/Apache-Kafka%EC%95%84%ED%8C%8C%EC%B9%98-%EC%B9%B4%ED%94%84%EC%B9%B4%EB%9E%80-%EB%AC%B4%EC%97%87%EC%9D%B8%EA%B0%80

## ELK(elasticStack)
* Elasticsearch, Logstash, Kibana의 세 가지 인기 있는 프로젝트로 구성된 스택을 의미하는 약어
* Elasticsearch라고도 불리는 ELK 스택은 사용자에게 모든 시스템과 애플리케이션에서 로그를 집계하고 이를 분석하며 애플리케이션과 인프라 모니터링 시각화를 생성하고, 빠르게 문제를 해결하며 보안 분석할 수 있는 능력을 제공한다

### ELK Stack의 구성
ELK는 세 가지 기술로 구성
* Elasticsearch  
  로그 저장 및 검색
* Logstash  
  로그 수집 엔진
* Kibana  
  로그 시각화 및 관리

세 모듈은 각자의 독립된 기술이기 때문에 필요에 따라 일부만 써도 된다. 하지만 서로 호환이 잘되고 합쳤을 때 시너지가 좋기 때문에 같이 구축된다. 세 기술을 같이 쓰기 때문에 Stack 이라고 부르며, 일반적으로 구성되는 형태는 다음과 같다.  
  
![image](https://user-images.githubusercontent.com/44667299/164011191-643706ee-a92a-4d36-98be-473df226f92e.png)  
  
* Data Processing (Logstash)
  * 서버 내의 로그, 웹, 메트릭 등 다양한 소스에서 데이터를 수집하여 입력
  * 데이터 변환 및 구조 구축
  * 데이터 출력 및 송신
* Storage (Elasticsearch)
  * 데이터 저장
  * 데이터 분석
  * 데이터 관리
* Visualize (Kibana)
  * Dashboard를 통한 데이터 탐색
  * 팀원들과 공유 및 협업하는데 사용 가능
  * 엑세스 제어 (Access Control) 사용 가능

하지만 각자의 모듈을 유연하게 대체할 수 있는 만큼, 실제 구축되는 형태는 다양하다.

### ELK Stack의 장점
* 가격
  * ELK는 Elastic이라는 회사에서 제공하는 오픈소스
  * 별도로 AWS EC2 등의 인스턴스에 사용한다면 관련 비용이 들어가긴 하겠지만, ELK 자체로만은 무료이기 때문에 다른 시스템에 비해 가격적인 측면에서 장점을 가짐
* 접근성 & 사용성
  * 오픈소스를 내려받아 설치하는 것으로 구축이 완료
  * 그 외의 별도의 추가적인 개발 과정이 필요없기 때문에 접근성이 뛰어남
  * 사용이 다른 유사 시스템들에 비해 쉬운 편
* 속도
  * ELK중 데이터 보관 및 분석 역할을 담당하는 Elasticsearch는 거의 실시간(Real-Time)에 가깝게 데이터를 처리할 수 있음
* 유연성
  * ELK는 각자의 기능을 담당하는 세가지의 모듈을 붙여서 만듬. 그렇기 때문에 그 기능만 담당할 수 있다면 얼마든지 모듈을 유연하게 변경할 수 있음
  * 예를 들어서, 굳이 Logstash를 쓰지 않아도 데이터 수집 역할만 할 수 있다면 경우에 따라 다른걸로 대체가 가능

### ELK의 변화, 최근 동향
**Beats 의 도입**  
기존의 ELK에서의 큰 문제점 중 하나는 Logstash였다.  
데이터 수집의 역할을 맡고 있는 Logstash는, 원하는 형태로의 데이터 입출력 변환 기능까지 맡고 있었기 때문에 그 오버헤드가 컸음  
  
많은 사용자들의 요구사항을 받아들인 Elastic사는, 오로지 데이터의 수집만을 담당하는 경량화된 모듈 Beats를 도입함  
  
![image](https://user-images.githubusercontent.com/44667299/164012402-5dd30d5a-b8f2-4ab9-ad61-d7e07090d749.png)  
  
Beats는 한 가지를 이르는 말이 아니며 위와 같이 수집하고자 하는 데이터의 유형별로 다른 모듈이 존재  

<br/>

**Cloud 기반의 Elasticsearch 도입**  
큰 규모의 회사들은 기존 On-Premise 기반의 서비스들을 클라우드 기반으로 마이그레이션하기 위해 발빠르게 움직이고 있다.  
ELK 또한 이런 흐름에 맞게 Cloud 기반의 Elasticsearch를 도입
>온프레미스(On-Premise) : 기업의 서버를 클라우드 같은 원격 환경에서 운영하는 방식이 아닌, 자체적으로 보유한 전산실 서버에 직접 설치해 운영하는 방식

참고 및 출처 : https://velog.io/@holidenty/ELK-ELK-Stack-%EC%9D%B4%EB%9E%80-%EB%AC%B4%EC%97%87%EC%9D%BC%EA%B9%8C

## 레디스
고성능 키-값 저장소로서 문자열, 리스트, 해시, 셋, 정렬된 셋 형식의 데이터를 지원하는 NoSQL
* 모든 데이터를 메모리에 저장하고 조회(인메모리 데이터베이스)
* 인메모리 상태에서 데이터를 처리함으로써 RDB나 문서형(Document) DB보다도 빠르고 가볍게 동작
* 다양한 자료구조(String, Set, Sorted Set, Hash, List)

다양한 자료구조를 지원하게 되면 개발의 편의성이 좋아지고 난이도가 낮아진다는 장점이 있다.    
예를 들어, 어떤 데이터를 정렬을 해야하는 상황이 있을 때, DBMS를 이용한다면 DB에 데이터를 저장하고, 저장된 데이터를 정렬하여 다시 읽어오는 과정은 디스크에 직접 접근을 해야하기 때문에 시간이 더 걸린다는 단점이 있다. 하지만 이 때 In-Memory 데이터베이스인 Redis를 이용하고 레디스에서 제공하는 Sorted-Set이라는 자료구조를 사용하면 더 빠르고 간단하게 데이터를 정렬할 수 있다.

### Redis 특징
* 프로세스로 존재
* In-Memory : 메모리 기반
  * 영속성을 지원하는 인메모리 데이터 저장소
* 키-값 구조 데이터 관리 시스템 : 비 관계형이며, 키-값 구조이기 때문에 별도 쿼리 없이도 데이터를 간단히 가져올 수 있음
* 읽기 성능 증대를 위한 서버 측 복제를 지원
* 쓰기 성능 증대를 위한 클라이언트 측 샤딩(Sharding) 지원
* 다양한 서비스에서 사용되며 검증된 기술
* 문자열, 리스트, 해시, 셋, 정렬된 셋과 같은 다양한 데이터형을 지원
  * 메모리 저장소임에도 불구하고 많은 데이터형을 지원하므로 다양한 기능을 구현

### Redis를 어디에 사용하는가
운영중인 웹 서버에서 키-값 형태의 데이터 타입을 처리해야 하고, I/O가 빈번히 발생해 다른 저장 방식을 사용하면 효율이 떨어지는 경우에 사용  
  
ex : YouTube 조회수에 해당하는 데이터를 RDS 형태의 데이터에 저장해 I/O를 반복한다면 엄청난 자원이 사용될 것  
이처럼 어마어마한 I/O를 발생시키는 데이터를 처리할 때 레디스를 사용해 데이터를 캐싱 처리하고, 일정한 주기에 따라 RDS에 업데이트를 한다면 RDS에 가해지는 부담을 크게 줄이고, 성능은 크게 향상할 수 있음  
  
**캐시를 사용할 때**  
서비스 사용자가 증가했을 때, 모든 유저의 요청을 DB 접근으로만 처리하게 된다면 DB 서버에 무리가 갈 수 밖에 없다. 물론 데이터베이스는 데이터를 디스크에 저장하기 때문에 서버의 장애와는 별개로 데이터를 유지할 수는 있지만, 요청이 증가하는 상황에서는 기존 성능을 기대하기 힘듦  
  
이런 맥락에서 캐시는 나중에 요청된 결과를 미리 저장해두었다가 빨리 제공하기 위해 사용  
  
Redis Cache 는 메모리 단(In-Memory)에 위치. 따라서 디스크보다 수용력(용량) 은 적지만 접근 속도는 빠름  
  
![image](https://user-images.githubusercontent.com/44667299/164019049-445f80e2-ea9d-443a-afb7-db36f3cf3e11.png)  
사진 출처 : https://velog.io/@hyeondev/Redis-%EB%9E%80-%EB%AC%B4%EC%97%87%EC%9D%BC%EA%B9%8C  
  
* 일반적인 패턴 : Look aside cache
1. 웹 서버는 클라이언트 요청을 받아서 데이터가 존재하는지 캐시를 먼저 확인
2. Cache에 데이터가 있으면 그걸 꺼내주는데, 만약 없으면
3. DB 에서 읽어서 → 먼저 캐시에 저장한다음 클라이언트에게 데이터를 돌려줌
* Write Back
  * 데이터를 캐시에 전부 먼저 저장 해놓았다가 특정 시점마다 한번씩 캐시 내 데이터를 DB insert 하는 방법
  * insert를 1개씩 500번 수행하는 것보다 500개를 한번에 삽입하는 동작이 훨씬 빠름에서 알 수 있듯, write back 방식도 성능면에서 뒤쳐지는 방식은 아님
  * 하지만 어쨌든 여기서 데이터를 일정 기간동안은 유지하고 있어야 하는데, 이때 이걸 유지하고 있는 storage는 메모리 공간이므로 서버 장애 상황에서 데이터가 손실될 수 있다는 단점이 있다. 그래서 다시 재생 가능한 데이터나, 극단적으로 heavy한 데이터에서 write back 방식을 많이 사용한다.

### Redis 영속성
* 메모리에서 데이터를 관리하는 방식(휘발성 기반)
* 특정 시간에 데이터를 디스크에 저장하는 방식(RDB 기반)
  * RDB(Snapshotting) 방식
  * 순간적으로 메모리에 있는 내용을 DISK에 전체를 옮겨 담는 방식
* 수시로 데이터를 디스크에 저장하는 방식(AOF 기반)
  * AOF(Append On File) 방식
  * Redis의 모든 write/update 연산 자체를 모두 log파일에 기록하는 형태

## 도커
컨테이너 기반의 오픈소스 가상화 플랫폼  
  
다양한 프로그램, 실행환경을 컨테이너로 추상화하고 동일한 인터페이스를 제공하여 프로그램의 배포 및 관리를 단순하게 해줌. 백엔드 프로그램, 데이터베이스 서버, 메시지 큐등 어떤 프로그램도 컨테이너로 추상화할 수 있고 조립PC, AWS, Azure, Google cloud등 어디에서든 실행할 수 있다.  
  
소프트웨어를 컨테이너라는 표준화된 유닛으로 패키징하며, 컨테이너는 라이브러리, 시스템 도구, 코드 등 소프트웨어 실행에 필요한 모든 것이 포함되어 있다. 즉, 도커는 컨테이너 환경에서 독립적으로 애플리케이션을 실행할 수 있도록 컨테이너를 만들고 관리하는 것을 도와주는 도구이다. 도커를 통해 애플리케이션을 실행하면 독립적인 환경에서 일관된 결과를 보장한다. 도커의 핵심 개념은 이미지와 컨테이너다.

### 컨테이너
컨테이너는 격리된 공간에서 프로세스가 동작하는 기술이다. 가상화 기술의 하나지만 기존방식과는 차이가 있다.  
  
기존의 가상화 방식은 주로 OS를 가상화  
  
VMware나 VirtualBox같은 가상머신은 호스트 OS위에 게스트 OS 전체를 가상화하여 사용하는 방식. 이 방식은 여러 가지 OS를 가상화(리눅스에서 윈도우를 돌린다던가) 할 수 있고 비교적 사용법이 간단하지만 무겁고 느려서 운영환경에선 사용할 수 없다.  
  
전가상화든 반가상화든 추가적인 OS를 설치하여 가상화하는 방법은 어쨌든 성능문제가 있었고 이를 개선하기 위해 프로세스를 격리 하는 방식이 등장  
  
리눅스에서 프로세스를 격리하는 방식을 리눅스 컨테이너라고 한다. 단순히 프로세스를 격리하기 때문에 가볍고 빠르다. 또한 CPU나 메모리는 프로세스가 필요한 만큼만 추가 사용하여서 성능적으로 거의 손실이 없다.
>도커의 기본 네트워크 모드는 Bridge모드로 약간의 성능 손실이 있음. 네트워크 성능이 중요한 프로그램의 경우 --net=host 옵션을 고려해야 함

도커(왼쪽)와 가상머신(오른쪽)  
![image](https://user-images.githubusercontent.com/44667299/164034399-73109600-cd20-4af4-bf77-f561b8341155.png)  
![image](https://user-images.githubusercontent.com/44667299/164034520-ff3ae64c-99d6-47fd-8adf-47530b1b3ffb.png)  
사진 출처 : https://subicura.com/2017/01/19/docker-guide-for-beginners-1.html  
  
**컨테이너의 특징**
* 서버에 여러 컨테이너를 실행하면 독립적으로 실행되어 가벼운 VM(Virtual Machine)을 사용하는 느낌을 줌
* 실행 중인 컨테이너에 접속하여 명령어를 입력할 수 있음
* apt-get이나 yum 등 운영체제에서 사용하는 패키지 매니저를 통해 설치할 수 있고 사용자도 추가하고 프로세스를 백그라운드로 실행할 수 있음
* CPU나 메모리 사용량을 제한할 수 있음
* 호스트의 특정 포트와 연결하거나 호스트의 특정 디렉토리를 내부 디렉토리인 것처럼 사용 가능
* 새로운 컨테이너를 만드는데 1~2초로 매우 빠름

### 이미지
컨테이너 실행에 필요한 파일과 설정을 포함하고 있는 것. 상태값을 가지지 않고 변하지 않는다. 컨테이너는 이미지를 실행한 상태이다. 추가되거나 변하는 값은 컨테이너에 저장된다. 같은 이미지에서 여러 개의 컨테이너를 만들 수 있고 컨테이너의 상태가 바뀌거나 삭제되어도 이미지는 변하지 않고 그대로 남아있다. 도커 이미지는 Docker hub에 등록하거나 Docker Registry 저장소를 직접 만들어 관리할 수 있다.  
  
**이미지의 특징**
* 도커 이미지의 용량은 보통 수백 MB ~ 수 GB이지만 가상머신에 비하면 작은 용량
* 상태값을 가지지 않고 변하지 않음
* 하나의 이미지를 통해 여러 컨테이너를 생성할 수 있고, 컨테이너를 삭제해도 이미지는 변하지 않음
* 이미지들은 Docker Hub를 통해 버전 관리 및 배포가 가능
* 도커는 Dockerfile이라는 파일로 이미지를 만듦

### Dockerfile
* 도커 이미지를 만들기 위해 Dockerfile이라는 파일에 DSL(Domain Specific Language) 언어를 이용해 이미지를 생성할 수 있음
* 단순 텍스트 파일로 일반적으로 소스와 함께 관리
* 서버에서 프로그램을 설치하려고 할 때 Dockerfile을 통하여 관리하면 된다.

참고 및 출처 : https://tecoble.techcourse.co.kr/post/2021-08-14-docker/, https://subicura.com/2017/01/19/docker-guide-for-beginners-1.html

## 쿠버네티스
컨테이너화된 애플리케이션의 자동 디플로이, 스케일링 등을 제공하는 관리시스템  
  
쿠버네티스는 Linux 컨테이너 작업을 자동화하는 오픈소스 플랫폼이며, 도커를 관리하는 컨테이너 오케스트레이션 툴
>오케스트레이션 툴
>* 컨테이너 역시 그 수가 많아지게 되면 관리와 운영에 있어서 어려움이 따른다.
>* 컨테이너 오케스트레이션은 이러한 다수의 컨테이너 실행을 관리 및 조율하는 시스템이다.
>* 오케스트레이션 엔진을 통해 컨테이너의 생성과 소멸, 시작 및 중단 시점 제어, 스케줄링, 로드 밸런싱, 클러스터링 등 컨테이너로 어플리케이션을 구성하는 모든 과정을 관리할 수 있음
>* 다른 컨테이너 오케스트레이션 툴로는 도커 스웜, ECS, Nomad 등이 있다.

### 쿠버네티스 특징
* 자동화된 복구(self-healing)
  * 컨테이너들을 모니터링하며, 컨테이너 중 하나라도 죽으면 쿠버네티스는 그것을 빠르게 재시작 시킴
* 로드 밸런싱(Load balancing)
  * 만약 1만명의 유저가 접속하지만, 당신의 웹/앱은 준비가 되지 않았을 경우 쿠버네티스는 해당 웹사이트의 니즈를 수용할 수 있도록 자동으로 새로운 컨테이너들을 만들 수 있다.
  * 또한, 니즈가 줄어들면 컨테이너의 숫자를, 지정해둔 최소 숫자로 자동으로 조절한다.
  * 이전에는 수동으로 했던 작업들을 쿠버네티스가 자동으로 도와주는 것
* 무중단(Fault tolerance-FT) 서비스
  * 기업에서는 서버 업데이트를 위해서 사용자들이 잠든 새벽 시간을 활용하거나, 긴급 점검의 형태로 서비스를 일시 중단해왔다.
  * 하지만, 쿠버네티스는 점진적 업데이트를 제공하기 때문에, 서비스를 중단하지 않고도 애플리케이션을 업데이트 할 수 있다.
* 호환성(Vendor Lock In 해결)
  * 고객이 A사의 클라우드를 사용하다가 I사의 클라우드로 환경을 이전하고 싶을 때, 서로 다른 업체(Vendor)의 클라우드 제품 간에 호환 문제가 발생하여 이전하기 어려운 상황을 Vendor Lock In 이라고 함
  * 쿠버네티스는 도커 컨테이너를 기반으로 하는 오픈소스이기 때문에, 사용자들이 특정 업체에 종속되지 않고 클라우드의 환경들을 이전할 수 있음
  * 또한, 한 번 쿠버네티스를 익히면 provider 회사에 상관없이 공통된 마이크로서비스 아키텍쳐 개발이 가능

### 도커 vs 쿠버네티스
도커는 통상적으로 컨테이너를 가리키는 개념이자 관리 도구이며  
쿠버네티스는 통상적으로 오케스트레이션을 가리키는 개념이자 도커를 관리하는 툴임  
  
도커는 하나의 이미지를 컨테이너에 띄우고 실행하는 도구이며  
쿠버네티스는 컨테이너를 동적으로 여러개의 컨테이너로 필요한 만큼 생성하거나, 독립적으로 분리, 독립시켜 마이크로서비스를 할 수도 있는 도구  
  
쿠버네티스는 로드밸런싱, 자동화된 복구, 무중단 서비스와 클러스터링 등의 기능을 제공  
컨테이너를 하나만 구성 관리하는 경우에는 사용할 필요가 없는 도구  
  
**비교 예시**
* 컨테이너를 하나만 띄워서 사용해야지! → 도커
* 0월 0시에, 100개의 컨테이너를 자동으로 생성해야지! → 쿠버네티스
  * 즉, 도커는 '이미지를, 컨테이너에 띄우고 실행하는 기술'이고
  * 쿠버네티스는 '도커를 관리하는 툴'이라고 생각
  * 따라서, 도커는 '한 개의 컨테이너를 관리'하는 데 최적화 되어있고,
  * 쿠버네티스는 '여러 개의 컨테이너를, 서비스 단위로 관리'하는 데 최적화 되어있다.

### 요약
* 도커와 쿠버네티스는 상황마다 다르게 사용됨
* 한 개의 컨테이너만 사용한다면 쿠버네티스는 필요 없음
  * 도커는 하나의 이미지를 컨테이너에 띄우고 실행하는 도구
* 쿠버네티스는 많은 컨테이너 관리에 유용
  * 쿠버네티스는 도커를 기반으로 컨테이너를 관리하는 서비스

출처 및 참고 : https://wooono.tistory.com/109, https://bangu4.tistory.com/142, https://cocoon1787.tistory.com/755

## 프로메테우스
* 메트릭 기반의 오픈소스 모니터링 시스템
* 이벤트 모니터링 및 경고에 사용되는 무료 소프트웨어 응용 프로그램
* 유연한 쿼리(PromQL) 및 실시간 경고가 가능
* 구조가 간단해서 운영이 쉽고, 강력한 쿼리 기능을 가지고 있으며, 그라파나(Grafana)를 통한 시각화를 지원
* ELK와 같은 로깅이 아니라, 대상 시스템으로부터 각종 모니터링 지표를 수집하여 저장하고 검색할 수 있는 시스템
* Go 언어로 작성되었으며 아파치 2 라이선스를 사용
>메트릭
>* 수집하는 시계열 데이터
>* 프로메테우스의 메트릭은 "메트릭명{필드1=값, 필드2=값} 샘플링데이터" 와 같이 수집됨

### 기능 및 구성
* 메트릭 수집, 시계열 데이터 저장
* 유연한 쿼리 언어인 PromQL을 통한 성능 분석
* 그라파나를 통한 데이터 시각화
* alertmanager를 통한 알림 생성
* 애플리케이션 코드 계측을 위한 클라이언트 라이브러리

### 아키텍처
![image](https://user-images.githubusercontent.com/44667299/164042440-7ab152fe-3f27-4a59-a7cb-5f5c71647df5.png)
* Pushgateway : Proxy Forwarding을 하여 접근할 수 없는 곳에 데이터가 존재하는 경우에 사용할 수 있는 대안. 애플리케이션이 Pushgateway에 메트릭을 push 한 후, Prometheus server가 Pushgateway에 접근하여 메트릭을 pulling 한다.
* Prometheus server : 프로메테우스의 메인 서버로 메트릭 데이터를 수집하고 저장한다. Prometheus server 내부에는 Retrieval, TSDB, HTTP server 모듈이 있다.
* TSDB(Time-series Database) : 수집된 메트릭은 Prometheus server 내의 메모리와 (default) 로컬 디스크에 저장된다. (필요에 따라 원격 서버에 데이터를 저장할 수 있다.) 데이터베이스에 별도로 저장하지 않기 때문에 대상 시스템이 늘어날수록 디스크를 늘려야 한다.
* HTTP Server : 프로메테우스에 저장된 데이터를 조회하기 위해서 필요한 서버. 프로메테우스는 데이터를 가져가기 위한 프로토콜로 HTTP REST API를 제공하고, 직접 API를 통해 데이터를 가져가든지, Web UI 대시보드에서 데이터를 조회하는 방법으로 그라파나를 통해 데이터를 시각화할 수 있다.
* Alertmanager : 프로메테우스에서 문제가 발생했다고 생각되는 시점에 slack, mail, hipchat 등을 통해 알람을 보내준다. 알람을 거는 기준은 Rule을 작성해서 load 시키는 방식으로 정할 수 있다.

### 장단점
**장점**
1. pull 방식의 구조를 채택함으로써, 모든 메트릭의 정보를 중앙 서버로 보내지 않아도 된다.
2. Kubernetes 환경에서 설치가 쉽고, grafana와의 연동을 통한 운영이 쉽다.
3. Linux, Window등의 OS metric 뿐 아니라 각종 Third-party의 다양한 exporter를 제공한다.
4. 데이터 저장소가 시계열 데이터 저장소로 구성되어있어, 많은 양의 정보를 빠르게 검색 가능하다.
5. 구조가 복잡하지 않고 간단하기 때문에 특정 솔루션에 대한 export가 어렵지 않다.
6. 모든 데이터를 수집하지 않고 일정 주기(기본 값: 15초)로 메트릭을 수집하기 때문에 애플리케이션에 무리가 없다.

**단점**
1. 클러스터링이 불가능하다. 프로메테우스를 여러대 구성하려면 아래 그림처럼 Hierarchy 구조를 구성하여 사용해야한다.
2. 모든 메트릭을 수집하지 않기 때문에 사실상 "추리"를 보는데에는 좋지만 APM(Application Performance Monitoring)과 같이 모든 로그를 추적하기에는 적합하지 않다. Pullling 하는 순간의 스냅샷 정보만 알 수 있다.
3. 싱글 호스트 아키텍처이기 때문에 저장용량이 부족하면 디스크 용량을 늘리는 방법밖에 없다.
4. Prometheus server가 다운되거나, 설정 변경 등을 위해 재시작할 경우 메트릭이 일정시간동안 유실된다.

![image](https://user-images.githubusercontent.com/44667299/164043531-ea1e0b63-9134-4525-9d17-2888ee9d215e.png)  
  
각 Region에 프로메테우스를 배치 한 뒤, 이를 Master에 Aggregate하는 방식이 프로메테우스가 공식적으로 권장하는 다중화 방식으로 Clustering과는 거리가 멀다.  
  
**주의할 점**
* 어디까지나 근사치라는 점
  * 일단 풀링 주기를 기반으로 메트릭을 가지고 오기 때문에, 풀링하는 순간의 스냅샷이라는 것
  * 15초 단위로 폴링을 했다고 가정 했을때, 15초 내에 CPU가 올라갔다 내려와서, 풀링 하는 순간에는 CPU가 내려간 값만 관측이 될 수 있다. 스냅삿에 대한 연속된 모음일 뿐이고, 근사값의 형태라는 것을 기억할 필요가 있다.
* 싱글 호스트
  * 프로메테우스는 싱글 호스트 아키텍처이다. 확장이 불가능하고, 저장 용량이 부족하면 앞에서 언급한 대로 디스크 용량을 늘리는 것 밖에 방안이 없다.
  * 특히나 문제점은 프로메테우스 서버가 다운이 되거나 또는 설정 변경등을 위해서 리스타트 등을 하더라도 그간에 메트릭은 저장이 되지 않고 유실이 된다는 점

참고 및 출처 : https://cumulus.tistory.com/86, https://bcho.tistory.com/1372

## 그라파나
* 오픈소스 메트릭 데이터 시각화/모니터링 도구로 메트릭 분석 플랫폼을 지향하고 있음
* 개발한 어떤 것에 대해 지표 현황을 쉽게 파악할 수 있게 해주는 대시보드
* 프로메테우스뿐만 아니라 엘리스틱서치, 인플럭스DB 등 여러 종류의 데이터 소스를 시각화할 수 있고, 높은 범용성을 가진 오픈 소스 도구

프로메테우스, 그라파나를 통해 비교적 쉽게 모니터링을 위한 프로세스들을 만들 수 있다. 여러 익스포터를 통해 필요한 매트릭 수집을 확장할 수 있다는게 큰 장점이며 PULL 방식이라 부하에 따른 장애나 성능감소를 걱정하지 않아도 된다. 그라나파를 통해 매트릭 데이터를 보기 쉽게 만들 수 있고 다양한 데이터 소스를 지원한다.

## 코틀린
JVM에서 동작하는 크로스 플랫폼 오픈소스 프로그래밍 언어. JetBrains사가 공개. 기존 자바 라이브러리나 프레임워크와 함께 잘 작동함

### 코틀린의 특징 및 장점
**대상 플랫폼 : 서버, 안드로이드 등 자바가 실행되는 모든 곳**  
코틀린의 주 목적은 현재 자바가 사용되고 있는 용도에 적합하면서도 더 간결하고 생산적이며 안전한 대체 언어를 제공하는 것
* 서버상의 코드(특히 웹 애플리케이션 백엔드)
* 안드로이드 디바이스에서 실행되는 모바일 애플리케이션

자바뿐 아니라 자바스크립트도 코틀린을 컴파일 할 수 있다. 따라서 코틀린 코드를 브라우저나 노드에서 실행할 수 있음  

<br/>

**정적 타입 지정 언어**  
자바와 마찬가지로 코틀린도 정적 타입 지정 언어임. 모든 프로그램 구성 요소의 타입을 컴파일 시점에 알 수 있다.  
**정적 타입 지정 언어의 장점**
* 성능 : 실행 시점에 어떤 메소드를 호출할지 알아내는 과정이 필요없으므로 메소드 호출이 더 빠름
* 신뢰성 : 컴파일러가 프로그램의 정확성을 검증하기 때문에 실행 시 프로그램이 오류로 중단될 가능성이 더 적어짐
* 유지 보수성 : 코드에서 다루는 객체가 어떤 타입에 속하는지 알 수 있기 때문에 처음보는 코드를 다룰때도 더 쉬움
* 도구 지원 : 정적 타입 지정을 활용하면 더 안전하게 리팩토링 할 수 있고, 도구는 더 정확한 코드 완성 기능을 제공할 수 있으며, IDE의 다른 지원 기능도 더 잘 만들 수 있음

<br/>

**Null Safe 언어**  
자바로 개발을 하면서 가장 많이 마주치는 문제는 NPE(NullPointException)이다. 코틀린은 Nullable과 Non-nullable 이라는 개념을 가지고 있어, null이 발생하지 않도록 만들어 준다. 그래서 자바 라이브러리와 함께 사용하지 않는 한, 코틀린으로만 개발할 때는 NPE와 같은 예외 문제가 발생하지 않음  

<br/>

**기존 라이브러리와의 상호운용성**  
코틀린은 기존의 자바를 보완하고 대체하기 위해 탄생했기 때문에, 자바와 100% 호환된다. 한 프로젝트 안에 자바 파일과 코틀린 파일이 함께 들어 있더라도 작동에 전혀 문제가 없다. 자바 기반의 라이브러리도 모두 문제 없이 사용할 수 있다. 또한 자바로 만들어진 프로젝트도 처음부터 코틀린으로 다시 개발할 필요 없이, 이후 생성되는 파일만 전부 코틀린으로 작성해도 전체 프로젝트가 정상적으로 작동하여 효율적으로 개발할 수 있다.

### 코틀린의 단점
* 속도가 느림
  * 안드로이드 어플리케이션을 위한 빌드를 생성한다면, 여전히 자바의 성능이 더 좋다. 컴파일 시, 자바로 변환하고 나서 바이트 코드로 변환하기 때문에 컴파일 속도가 상대적으로 느린 편
* 자바와 비교하면 학습 생태계가 작은 편
* 자바 문법에 익숙할 필요가 있을수도 있음
  * 자바로 개발한 프로젝트를 코틀린으로 이어서 작성하더라도, 자바 문법에 익숙하지 않다면 자바 기반의 기존 라이브러리를 완전히 이해하지 못하고 사용할 수 있다. 아직까지는 코틀린 기반의 전용 라이브러리보다는 자바 기반이 훨씬 많기 때문에, 코틀린으로 안드로이드 개발을 시작하더라도 프로젝트 개발이 심화될수록 자바에 대한 이해가 필수적으로 요구될 수 있다.

참고 및 출처 : https://incheol-jung.gitbook.io/docs/study/kotlin-in-action/1, https://media.fastcampus.co.kr/knowledge/dev/kotlin/
